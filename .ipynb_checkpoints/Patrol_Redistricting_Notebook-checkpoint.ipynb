{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough of How to Create Optimal Patrol Areas\n",
    "\n",
    "*By Andrew Wheeler*, [apwheele@gmail.com](mailto:apwheele@gmail.com)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial shows how to construct the p-median model, using python and the [pulp linear programming library](https://pythonhosted.org/PuLP/). If you have any questions and would like to apply this model to your own data, always feel free to email me with questions. See the header for my email address.\n",
    "\n",
    "First I will show how to construct the model, as well as the inputs needed in a step by step fashion for the simplified example of four atom areas. Then I will walk the reader through a more realistic example, using data from Carrollton, Texas to show how to apply the model to real world data.\n",
    "\n",
    "## Using pulp to estimate the p-median model\n",
    "\n",
    "So assuming python is set up and the pulp library is installed (if not, read the Extra section at the end on *Python Notes*), we first start our session by importing the pulp library, and then defining the data we will need to use to replicate the simple example provided in the article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data needed to set up the p-median problem\n",
    "\n",
    "from pulp import *\n",
    "\n",
    "Areas = ['A','B','C','D']\n",
    "\n",
    "Calls = {'A': 4, \n",
    "         'B': 1, \n",
    "         'C': 2, \n",
    "         'D': 3}\n",
    "\n",
    "Distances = {'A': {'A': 0, 'B': 1, 'C': 2, 'D': 3},\n",
    "             'B': {'A': 1, 'B': 0, 'C': 1, 'D': 2},\n",
    "             'C': {'A': 2, 'B': 1, 'C': 0, 'D': 1},\n",
    "             'D': {'A': 3, 'B': 2, 'C': 1, 'D': 0}}\n",
    "\n",
    "Contiguity = {'A': ['B'],\n",
    "              'B': ['A','C'],\n",
    "              'C': ['B','C'],\n",
    "              'D': ['C']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this code above creates four different objects: \n",
    "\n",
    "  - `Areas`: A list of atom areas that are smaller than the aggregate areas we want to cluster \n",
    "  - `Calls`: A dictionary of the total number of calls (or the weight of calls) within each atom area\n",
    "  - `Distances`: A dictionary of the distances between each atom area. These can be network distances, or similarly the time it takes to travel between two atoms \n",
    "  - `Contiguity`: This identifies whether two atoms touch each other (via a dictionary that contains a list)\n",
    "  \n",
    "If you want to extend this analysis to your own data, you will need to provide these input datasets, but with your own areas. I show later on (in the section *Example from original data*), how to create some of these files from a base set of data using ArcGIS. \n",
    "\n",
    "Next we will need to define several constants. These will be determined based on your particular needs when constructing areas. These constants are the total number of areas you want to construct out of the smaller atoms (here two areas), and the maximum amount of call weight that can be in any particular area. The total number of calls in this example is 10, which needs to be divided into two areas. Setting the disparity to 20% then sets the maximum number of calls an area can have to `5*1.2 = 6` in any solution. (Later on I introduce minimum call constraints as well, so the minimum number of calls can only be `5*0.8 = 4`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICKING THE CONSTRAINTS\n",
    "\n",
    "#Sets the number of areas to cluster\n",
    "TotalBeats = 2\n",
    "\n",
    "#This says what the maximum amount of allowable calls in a particular area\n",
    "SumCalls = sum(Calls.values())\n",
    "Disparity = 0.2\n",
    "MaxIneq = (SumCalls/TotalBeats)*(1 + Disparity) #20% disparity allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to set up the p-median model using the pulp linear programming library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 set up the problem\n",
    "ChoosePatrol = LpProblem(\"Creating Patrol Areas\",LpMinimize)\n",
    "\n",
    "#2 create decision variables\n",
    "assign_areas = LpVariable.dicts(\"Sources and Destinations\", [(s,d) for s in Areas for d in Areas], lowBound=0, upBound=1, cat=LpInteger)   \n",
    "\n",
    "#3 set up the function to minimize\n",
    "ChoosePatrol += lpSum(Distances[s][d]*Calls[d]*assign_areas[(s,d)] for s in Areas for d in Areas), \"Minimize weighted travel\"\n",
    "\n",
    "#4 Constraint - maximum number of beats\n",
    "ChoosePatrol += lpSum(assign_areas[(s,s)] for s in Areas) == TotalBeats, \"Setting Max number of beats\"\n",
    "\n",
    "#5 Constraint - No offbeat if local beat is not assigned\n",
    "for s in Areas:\n",
    "    for d in Areas:\n",
    "        ChoosePatrol += assign_areas[s,d] <= assign_areas[s,s], \"No off-beat if local beat is not assigned, %s %s\" % (s,d)\n",
    "\n",
    "#6 Constraint - every destination area must be covered at least once\n",
    "for d in Areas:\n",
    "  ChoosePatrol += sum(assign_areas[(s,d)] for s in Areas) == 1, \"Destination Area %s must be covered at least once\" % (d)\t\n",
    "\n",
    "#7 Constraint - no source area should have too high a workload\n",
    "for s in Areas:\n",
    "  ChoosePatrol += sum(assign_areas[(s,d)]*Calls[d] for d in Areas) <= MaxIneq, \"Source Area %s needs to have less call weight than %s\"% (s,MaxIneq)\n",
    "\n",
    "#8 Constraint - areas need to be contiguous\n",
    "for s in Areas:\n",
    "    for d in Areas:\n",
    "        ChoosePatrol += lpSum(assign_areas[(s,a)] for a in Contiguity[d]) >= assign_areas[(s,d)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has comments on what each line is doing. Each line corresponds to the same mathematics that I discuss in the article, and this shows how to set up the initial p-median problem, as well as how to formulate the constraints. Now we can solve the model, and return the results into a list to further process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is Optimal, The total weighted travel is 3\n",
      "\n",
      "Source,Dest,Distance,Calls,Calls*Dist\n",
      "('A', 'A', 0, 4, 0)\n",
      "('A', 'B', 1, 1, 1)\n",
      "('D', 'C', 1, 2, 2)\n",
      "('D', 'D', 0, 3, 0)\n"
     ]
    }
   ],
   "source": [
    "ChoosePatrol.solve()\n",
    "print(\"Status is %s, The total weighted travel is %d\" % (LpStatus[ChoosePatrol.status],value(ChoosePatrol.objective)))\n",
    "results = []\n",
    "for s in Areas:\n",
    "    for d in Areas:\n",
    "        if assign_areas[(s,d)].varValue == 1.0:\n",
    "            results.append((s,d,Distances[s][d],Calls[d],Calls[d]*Distances[s][d]))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Source,Dest,Distance,Calls,Calls*Dist\")\n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the atoms A and B were clustered into one area (with a source area A), and C and D were clustered into another (with source area D). The last three numeric values in each tuple are the distances between the source and destination (which again can be in terms of network distance or time to travel), the total number of calls in the atom (or the weighted calls), and the weighted travel (which is just the multiplication of the prior two). \n",
    "\n",
    "If you are interested in writing the results of the model to a subsequently file, which you can then use to create the patrol areas in say ArcGIS, you could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os \n",
    "\n",
    "#Write to csv file - change this to a path on your computer\n",
    "DataLoc = r'C:\\Users\\apwhe\\Dropbox\\PublicCode_Git\\PatrolRedistrict\\PatrolRedistrict\\DataCreated'\n",
    "os.chdir(DataLoc)\n",
    "\n",
    "with open(\"PMed_Simple_Results.csv\", \"w\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([('source','dest','dist','calls','cost')] + results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you import the csv into ArcGIS, then table join it to the original atoms, and then dissolve based on the `source` variable, that will create your new patrol areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a simplified function\n",
    "\n",
    "Instead of writing that from scratch, I have created a simple to use function. One additional aspect of this function is that it allows you to set a distance threshold with which to limit the decision variables. This will make the problem run much faster when you have many micro atoms with which you are trying to cluster. This function then just takes care of the necessary data management so you are not specifying constraints for decision variables that do not exist. \n",
    "\n",
    "Also this function introduces additional minimum equality constraints, and adjusts the contiguity constraints to only count areas that are closer to the source. Closer counts either in terms of distance or in terms of being in the direct path back to the source (imagine a J shape with the source at the crossbar, some parts in the curve will be further away, but still in the path back to the source). To do this I need to rewrite the function in terms of both X and Y decision variables, as well as turn the contiguity matrix into a networkx object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is Optimal, The total weighted travel is 3\n",
      "[('A', 'A', 0, 4, 0), ('A', 'B', 1, 1, 1), ('D', 'C', 1, 2, 2), ('D', 'D', 0, 3, 0)]\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "import networkx\n",
    "\n",
    "#Function to return results for p-median problem\n",
    "#Ar - list of areas\n",
    "#Di - distance dictionary, where Di[s][d] is the distance between source area a and destination d\n",
    "#Co - contiguity dictionary, where Co[a] returns a list of areas adjacent to area a\n",
    "#Ca - call dictionary, for Ca[a] returns the expected number of calls in area a\n",
    "#Ta - total number of areas to partition set into\n",
    "#In - exceptable level of inequality between loads, so passing 0.1 would mean top could have \n",
    "#     (Total Calls/Ta)*1.1 total calls in the partitioned area\n",
    "#Th - threshold to consider source to destination as viable decision variables\n",
    "\n",
    "def PMed(Ar,Di,Co,Ca,Ta,In,Th):\n",
    "    #Setting max/min number of calls allowable in a created area\n",
    "    SumCalls = sum(Ca.values())\n",
    "    MaxIneq = (SumCalls/Ta)*(1 + In)\n",
    "    MinIneq = (SumCalls/Ta)*(1 - In)\n",
    "    #Turn contiguity Co into networkx graph\n",
    "    G = networkx.Graph()\n",
    "    for i in Ar:\n",
    "        for j in Co[i]:\n",
    "            G.add_edge(i,j)\n",
    "    #Creating threshold vectors for decision variables\n",
    "    NearAreas = {}\n",
    "    Thresh = []\n",
    "    for s in Ar:\n",
    "        NearAreas[s] = []\n",
    "        for d in Ar:\n",
    "            if Di[s][d] < Th:\n",
    "                Thresh.append((s,d))\n",
    "                NearAreas[s].append(d)\n",
    "    #Set up the problem\n",
    "    P = pulp.LpProblem(\"P-Median\",pulp.LpMinimize)\n",
    "    #Decision variables\n",
    "    assign_areas = pulp.LpVariable.dicts(\"Sources and Destinations\", [(s,d) for (s,d) in Thresh], lowBound=0, upBound=1, cat=pulp.LpInteger)\n",
    "    y_vars = pulp.LpVariable.dicts(\"SourceAreas\", [s for s in Ar], lowBound=0, upBound=1, cat=pulp.LpInteger)\n",
    "    #Function to minimize\n",
    "    P += pulp.lpSum(Ca[d]*Di[s][d]*assign_areas[(s,d)] for (s,d) in Thresh)\n",
    "    #Constraint Max number of beats\n",
    "    P += pulp.lpSum(y_vars[s] for s in Ar) == Ta\n",
    "    #Constraint No offbeat if local beat is not assigned first line\n",
    "    #Second part is contiguity constraint\n",
    "    for (s,d) in Thresh:\n",
    "        P += assign_areas[(s,d)] - y_vars[s] <= 0\n",
    "        #Only do this if source and destination not the same\n",
    "        if s != d:\n",
    "            #Identifying locations contiguous in nearest path\n",
    "            both = set(networkx.shortest_path(G,s,d)) & set(Co[d])\n",
    "            #Or if nearer to source\n",
    "            nearer = [a for a in Co[d] if Di[s][a] < Di[s][d]]\n",
    "            comb = list( both | set(nearer) ) #Combining all nearer areas, should always have at least one\n",
    "            #Contiguity constraint\n",
    "            P += pulp.lpSum(assign_areas[(s,a)] for a in comb if a in NearAreas[s]) >= assign_areas[(s,d)]\n",
    "    #Constraint - every destination area must be covered at least once\n",
    "    #Then Max/Min inequality constraints\n",
    "    for (sl,dl) in zip(Ar,Ar):\n",
    "        P += pulp.lpSum(assign_areas[(s,dl)] for s in NearAreas[dl]) == 1\n",
    "        P += pulp.lpSum(assign_areas[(sl,d)]*Ca[d] for d in NearAreas[sl]) <= MaxIneq\n",
    "        P += pulp.lpSum(assign_areas[(sl,d)]*Ca[d] for d in NearAreas[sl]) >= MinIneq*y_vars[sl]\n",
    "    #solve the model, note if you do not have CPLEX change this to whatever solver you want\n",
    "    P.solve(pulp.CPLEX())\n",
    "    stat = pulp.LpStatus[P.status]\n",
    "    if stat != \"Optimal\":\n",
    "        print(\"Status is %s\" % (stat))\n",
    "        return stat\n",
    "    else:\n",
    "        print(\"Status is %s, The total weighted travel is %d\" % (stat,pulp.value(P.objective)))\n",
    "        results = []\n",
    "    for (s,d) in Thresh:\n",
    "        if assign_areas[(s,d)].varValue == 1.0:\n",
    "            results.append((s,d,Di[s][d],Ca[d],Ca[d]*Di[s][d]))\n",
    "    return results\n",
    "\n",
    "#Illustrating the function with our simple data\n",
    "simp_res = PMed(Ar=Areas,Di=Distances,Co=Contiguity,Ca=Calls,Ta=2,In=0.2,Th=100)\n",
    "print(simp_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in my function I use the CPLEX solver, instead of the default COIN installed with pulp. If you do not have access to CPLEX you need to change the line `P.solve(pulp.CPLEX())` to whatever solver you have access to. (I was not able to figure out how to pass this in as a parameter to the function, pulp would complain it could not find the solver if I changed the working directory.) See the pulp documentation about solvers. If you do not have any additional solvers installed you can just change this line to `P.solve()` and it should work. \n",
    "\n",
    "# Example from original data\n",
    "\n",
    "Since the python function I created uses such specific data sources as the inputs, you are likely wondering how do you exactly create them? I will show how to use ArcGIS to help create some of the original inputs, in particular the distance matrix and the contiguity matrix that are necessary inputs for the function. Here I start with just three pieces of data:\n",
    "\n",
    " - Geocoded calls for service\n",
    " - A set of micro atoms locations\n",
    " - A street network, preferably with speed limits\n",
    "\n",
    "From these three I show how to create the additional necessary pieces -- a set of micro atoms, a distance matrix of the times in between those atoms, and a contiguity matrix. These files are available in the `OriginalData` folder I have provided with the supplementary materials. The `CreatedData` folder and its contents are what I will show you how to make in this tutorial. \n",
    "\n",
    "If you do not have access to ArcGIS, I would suggest the open source QGIS as a suitable replacement. This tutorial also requires the network analyst extension for ArcGIS. If you do not have that there are tools within the CrimeStat freeware program that can calculate network distances as well. If you wanted to stay entirely within python, I would suggest checking out the [OSMnx library](https://github.com/gboeing/osmnx), although I do not think the open street map data will have speed limits in general.\n",
    "\n",
    "## Aggregating call weight to micro atoms\n",
    "\n",
    "First, in Carrollton they used a micro level named *Reporting Areas*. Because they use these areas in their CAD system and for various reporting purposes, they wanted any resulting patrol area solution to also conform to these boundaries. In the case that you did not have a micro set of areas to choose from, you may use census blocks or blockgroups, or you may create an arbitrary grid. Creating an arbitrary grid has the benefit that you can control the level of resolution of the micro atoms to cluster. (My experiments suggest you can estimate the models with up to around 1,200 micro atoms, but it takes awhile.) Here there are 300 micro atoms, which results in cluster solutions relatively quickly.\n",
    "\n",
    "So first open up ArcMap (into a blank map). Then use the Add Data button to import our two geographic files; the street network, and the reporting areas.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/ztqic1i5CE3wLmbiEoDyuFp0I99CoPKEJ_QqcpC-DCBzw129zBFmHKLmZ0OTIfMN_DvoQZvZao4Iny9S2ZW7JKOmICbn4siwFyZRfLXPOdEfCf8eLFp84MJmUpmt4q45dGpb5Acd79d8_BSAOYCUmghQID1pUlAJLrrAmDtPrAsaVeMjxxwEgKKEAvRupo8G7xJ2V0mOuYKWDiwqAGGxzhC7jpUhxFioyrq5yitdReUtK7ko09U3HkOhJ6_tqtk8FhzWwBl29E6YOSdQotgrGTCHQoMAbpTL8ESAGPHLxL1jxPuQc1lOteY_LAikVvH1znd2hoSVs6BW136fDfI1pog8yQGeJXnrKXXleqK3qdvEzytwWeTlZ0iDQKugqibAueULDs6bfj0W4Y4SNv_WfbeIvdDaogZJcoenE4oOs1kbl4jnv3Qs0PijD8086I9_zIY-NEGGNmQzVGcTBQ49F9DWYtQqaS39CgNyeVzikc2G6zeiw0Xlm--Vr40mZAyI8U8Ksa3wOjZm3Y8qIweQnd8AhNm67iIQYCmSMrksIRNrcBPQq7IbtmWsTk5e8k5YNHdQJrKV7ni1Ek7OB1jXTQLwpH4vHerj-IU7J0lhxg=w1236-h513-no)\n",
    "\n",
    "If you do not know how to load data into ArcMap, or how to map a network drive to where your data are located, I would suggest picking up a more basic tutorial on getting started in ArcMap, such as Gorr and Kurland's *GIS Tutorial 1* book. Once you have loaded the data, the map should look something like below (although your colors may be different). Remember to periodically save your map. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/R28ia7MC-aTCPzODVv6_TLe6__ZS4pNWFmkSmtMHJLhjvLJ-O78UmmfnzBlv4zLS8vMSyw9ZDuNDlK_qiFEPZ49UqTXuQVrYMi4Nrk95n8T2Dxz5kTLbTLPFJnQuF-_2jyuXM4Y4NYwKJ47lxQebr51zUODqlFG8M21hRLPZSBkoNOrJgVRyDJX5z9GAw2Z_pN1DaBTJBVivKS8x4L4pnbFqPO9xsAQQUhkFxw3c9XfZxzxZ1B57OeWESsyozHOjOLfynSHRuMLJGMtLVV7fuAvhq0MKAsGqVSOsaipAq8JHiOrOknT_x7ymosz9FCnfwvu6bIxX99PX9_DMg1JKLVFG2glOndy8Gvc90V6sUNYVmetyXFE1uDWYMJGN1ozv8tpkx4xtvNcoUVeTE_VqRuqaSfNXvTUw2-jISHetrL3wZcqWJ4G5d9skSHCbSdc5vJXJxQ4gCHBVWZ4jWdCC0-1Hzw9YMsaZ229abHIUkLw4gUyIoW1wWODZOCdlbAJBf3xjHRKFeAi27Kp9vXGgR0Zd9XDiNwUkyfs5exR3EzQ1MNunafnPKqGJDu6G76mXHBs2ERn-6ZpdlYIZoDDb_10hwPVMbtYIifePRtINCA=w1043-h858-no)\n",
    "\n",
    "Next we are going to load in the call data. Hit the Add Data button again, and then navigate to the CAD_DumpEdited.xlsx file. Double click on that file, and select the `Sheet1$` worksheet to add into the map. (Note if you get errors opening up an xlsx file, you may need to download the [driver connection here](https://www.microsoft.com/en-us/download/details.aspx?id=23734) and install that. If you cannot get the excel files to work, you can either save as a csv file or save as an earlier version of Excel as well.) \n",
    "\n",
    "![](https://lh3.googleusercontent.com/Ff-3hejBLAlOKI9bdZEdk37RfgXv6-V5vs9CkJGopWqxPw3MTnja8pZtZRu__ov3GUV9stneoSdYNZNUwK0epOj6JevkmJy8Gl12QY8JfROG8fryGC5O9Pa381j_FNEBUG8nw83M9k_PlE3sjp_pIAFGpbZnGwcyr4fMO7DIPiByisenebvfWp64VegXhde08mV80CVngWScb6P64Zfcvi0E8A19PaRjT-D9PUH4rYK6or8cvwZLxhbSjR4CFiiohVPIjHStG6tajMMrHK8zlcFkocfcZ3gv72vb56Hdj4iUkEBgSVcNzEO2nTBFKY2Cx6hSeN9KSd7PFpiwmAPNNp06er7qfjV-QXT9tcfnD9weLIsAzTA6xbL97gBx8E0Ympfp6q2--NUZi03kqXh8W--aEzt1zd4OXD22Vdj_b5uusx_8NyBCbeJMeD-50waVr3hkLXHuRKokjhG5SzDPeA_ziygiW0QdSkS6S3Q0bNfj8WA5GUdRkdplyeh-HsZbc3miKJoFozB5PJcvBIqjF8pNzpO34YwO0V56Rh8h_FmdmVwXusY7G1w0qg-h8aHIZO5UjN1RiO966BiP9D53Qg9eE9anvi_4GDmQhRCg_w=w1093-h579-no)\n",
    "\n",
    "Now right click on the table that was just imported into ArcMap and select Open. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/tXGB8l4ZFKwM8Df7vtD_lEljxmFQcXvC18f-ov6x_gxnEkWgBhkMU6-07TNYiWUdbTvzJaFs7KZgmOdph5X1R8nuge4bbFardHYduudz0fISk7lvYQ3q4gHOqYcf5udE6Yd_-9DIsRDkFAsThLDytGMpdTISqTXDHIXalHNMcZ4P3M7fNhhQ3zFyyegNrSDRF520DpnY6qZIfBt-eKFewrBWdnX1CD16kEyEAJXQDoH7kLGaLLxLToL23dWiy9JFCziyP1836Gk2zcgLG6q--OIhGYz7SAsHSvn2IzdN7cOAjxRHS2QO463mItDm_W9sJtwBVracN5V0jWn9cDxfbQvZN7a6UxTkv2piax1duUjxo0bT699rBv-caNKVL-I86szveQA2_wwy3VfWglbOq9CxF3qgsXN3tX0nJPqZdMESJwsQZEr6PPbHRljFjY193h5dTypMUUBj4NVdMpMtOjumax8TqK7RfhMtSkh0XtwSUy9nMl_tjtNPOFlMqtUp8L77Mhl9Ch_qZt6rWNRd1dvar65nXyM5PydnXUB2TFN9dlEpSpYZ8iFw-gc-G9mIInDG_qz5IoczgvtzOmEqC_azLw_m1HLuarzid9tpmw=w456-h460-no)\n",
    "\n",
    "You will see four fields; ra, beat, MinutesClear, and MinutesClearCapped. Here we want to sum up the number of calls and/or the call weight (determined by how long it takes to service a call), up to the reporting area (RA) level. To \n",
    "\n",
    "Now right click on the ra field and select Summarize. (In the case that you do not have a field in your data already, you will need to geocode your CAD incidents, and then do a spatial join from the incidents to the greater area you want to aggregate. That will give you the aggregate field you need.)\n",
    "\n",
    "![](https://lh3.googleusercontent.com/qCbYX7UE5uaddbKQ9sL58nVAW43IuEazKuyB5IA-H7IS1TidPmI_UoUwYghiNJlQHgSxdB9Tvq0Kya33xEmzqLLAJjcJo1YizRuwbtzJ4-u5lXvgAu1M66gKNvkZ38-C7g1aYVohXWTXgw9PELi4t00tI-5jGGQ2zZ5B5ILICf7noeEWmrXBRMQywqeEOl2M36vgN4wDJB_Ja7frqxazrW6oDfQGOf0lHgImDK5K2kTXrZgpOqkpuUH8JHoN0HcA706Y6Uw-ErjzdyB87qs_cKSH92_XUTaszVRAX_-TV5hyAvu3FvDffJJM8eEN96NRMgTEbBAipML4Q7JwKU9h5x6QJ4ygvP_jnLlK1uToF1SxUfSIijC3MUH56opwQJIb5xz45gH53BCIxAF_bhl0RNNDzxDDiLPiG8wWolGJqIYnvssqF1keF664WLWEcBPurwivl002caORCgy_Xy1wW8Se7nR-ud4oUnXMzcQRVd3FeobKF_Gk-gHMX5TXuESOy8keylfukVIABDJp4j38FWp3-unLf_RpD1Lno_1nC_XbYb00k6KCoze8goBz4ic0I7nRogjvUeGcZ2kBPwZPRuXswQkPIaQTWdc4vdnS3A=w452-h400-no)\n",
    "\n",
    "In the dialog that pops up, expand both MinutesClear and MinutesClearCapped, and check the box Sum. Then specify where you want to save the resulting aggregated table and its name. Here I name the table `RA_Calls`.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/dfek3x7QdnYll9X97Gqj8KULoRI1Hvrj9zPXvvOzw7vDUaGZXhjiIndnX9sZlatamw3W7Lb-THZ7Byux2q1ikzDmv7Y33YDn2fQgzZ3CbfhZ-JS_TQ2P940CcfhTCo0tucwzNLhVSIce45gkBqLBJZzxIa0WHegkPkeHUGd_eI1sfQvmw-yBg1eLkSYRo46bVxRkoif8rK7HFgXIJdA8oMKwVFRuFbX1wzx67Gi016SQ5v5XpdBreDLmrx8ngG6ts9wJ8nanrcAiTDHvh54lttqOtM7ivihLvU-9gCXg3FSdT3sHPo4HDzaB_CBxn5pvAaDTh8xP22mNJFkQ7dClM5xWy9CrtxpYY2Kvv988D-mz0vay1rhi3LxIVIU5sSmoeJl1wyGpeT4RYNyMLZD7OTRMonwly3IZhD4PFYwWsoiZ4C5UZBdh_LcUM8_cup8I5ZFz-0RVE7b-CwsEIru0BQjVxSOWQQ6RysKfzax1nGZ_5kDAQuez0XoFPI77MhPPm_Zn6Tw1yftF-ZnpbjsQdVquvreRVkB0uqc3j8mddzx7SApEbjrCPwcOBL6oXho80t35FGbPoN2vE5TpVqfbR8qW0J6fdlV3D4EltrjOLQ=w412-h500-no)\n",
    "\n",
    "After that is done click Yes to add the table to the current map. If you right click and open up the new table, you will see five fields; OID, ra, Count_ra, Sum_MinutesClear, and Sum_MinutesClearCapped. These fields provide the total number of calls in an area, as well as the weighted number of calls, according to how long they took to service (from the time the first officer is on scene until the call is listed as cleared). Here I have the capped field, as some calls have abnormally long times to clear. Here I capped the maximum minutes to 240 (4 hours), but that was arbitrary. Additionally I also removed incidents with missing time clear. These are sometimes auto-populated in databases as 1-0-1900, so they can create large negative times if you do not remove them.\n",
    "\n",
    "To see how correlated in this dataset using just the count vs the weighted calls is though, click on the list icon in the top left corner of the table, and select Create Graph.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/udL2DknafVjIIajgy1tp0GZtAzWtFqYOpgebsDZ3QTyN8UnPWlVWINAwrYX4CQ4VFMkwuR4vRM_3fhCyD6crTqYy6L0iHSCHQmIgkvrowDsr8hsAYuYbzqkG3hGKP7LcTLehZpswLgdskoSzswBfoWZk7tjlHUqiwwhbflKBSfchllIMrxs_FzrIH0M57EBG5K4PE66jV7xKREWeYNcZ7vSGaqOfi_FGlc5pFmZvn61MKwdfTTZI-hGLax3bUF1brucSvZuVMXTJuMS9wuT87WJcqo4UAjRh2oKhOdMOiDvW-PSqQAL7bT6KM6HL4Qxn-dLAy560zeUfrjsjhNmC7PdJZ9DzXmJAgSVohCMqytvEw2WJG9X_DFD1XT9FeYvh0X09AVWra_UvXNNx-ZAqg_MHFsSbRq8DYKPdd8HkG7sK2sammdQ6Mydu1lY0N15lmHR3uA1xWNgm8gPZwM_fqWDqP8CHDVTuygiDnnu9AiZNpXAdiBOFd_H_tQub1oeN4qZ4q3BOP2XvU-L99gMxiAPBroLbTjMUyB5Tw-dfIFD9SOQqHU6MbdRK6g3BFpy576G3fe9Mdx4naSalNJtucsIv3lakqTf5oVwlq7qt_Q=w448-h556-no)\n",
    "\n",
    "Then go through the options to create a scatterplot. \n",
    "\n",
    " - Graph type: Scatter Plot\n",
    " - Y field: Sum_MinutesCapped\n",
    " - X field: Count_ra\n",
    " - Click off legend\n",
    " - Change the Symbol properties to Circle, and change the Color to light grey\n",
    " \n",
    "After those changes your chart should look like below:\n",
    " \n",
    "![](https://lh3.googleusercontent.com/EsgsgzRCIL0wZPkuxax7QQmL7bKllmr3w0-T8C8VMGMSU2DX3jaY7tnJEWTkUHkNaCILy8C29bpFn-pDWR02Ir_Ayb3zNMMx177WFi3rqbsldCTxhCE4dq47jdlw3RqXgRuumRpxrefL69rW-kwZo0hScYHjI-FMM5GxTxQ3TA1_KCmYgl-0Q9-9LKj0Hv8zU6S-Py7DvYFTQ6UhMc2aSYQrYjfQ6KpMn8mWLDiqgDDz221JcWL7KPhy02QBmZafJcQHvmMSDBQwzNPMLNjLNZcDs_cVoxAoTutNKzwlUjBnBIQvq8acG8Dmqa5FV2fiAr2INvSQCtOfqwSzXzuzLwE0v527YTgD7PfAo2fNv96bZpUdfAQ-4aJ_rOWyeR-NsDLsc3zRGOwAd6ZFhazel4wV4LKUSxKz-fQs9mFYlmyrNprDSQPxZjjYbLF7Ou_-6phwCkt_RNXJaXGFgsxF_N_fviRuAvbYA5MUo0Yp2i3Iw72CSUnKKu2HrT7AiQcKQ3n6ypdtUN1hOL8tmGFbzq84_GWh17L_Q9FBdcpb_tfQ7CySX_HqWprdr0nhGOzN-Hb4nqTfiCpQ_RWZNrbFqThprlH8a3gfjru4Y1WGyg=w973-h534-no)\n",
    "\n",
    "Then if you click Next you can finish making the chart nicer by editing the axis labels and the overall chart title. You can clearly see on the chart one outlier -- which ends up being calls that were not in an assigned reporting area. Here is what the chart looks like when removing those no RA calls:\n",
    "\n",
    "![](https://lh3.googleusercontent.com/94m1162Av8T9u2UnYE76_RDD8jgbvT0L4GqKBbX7IR9Bu_c_MvHc7xq6aaknReaFanhRxLVnuDQ-1D37j7sqz2eH3Sw3L1LdIE9XXvxtL0BiaffICbr5omu57Eyq8UtfmeNhPEGyEmkVWWHstx38Nm0L6Mmaz0XXIKn2hz-DhG8mI9c1HrqspKEYaAlXtm1GwXIH83Is-iuM-FdU3huf15R-GSzbBdf7Ra3pxEy8AYUcsWScoJfET959oE_AV0fui7il4Td5AXuReBBwZ2EnlXbbYqvrQBRX00aeu8h43JySkCFuvhJDlrIkFXYtz0GBOFni7ZoI2KzRejs6KzaoCgMZ2jZ5xjOtEALwqDq7-7qY2kI8yD_a3RSWFIzAWdmijRJ-iPxuQ3kPCHllqrt8KXl0s2GvGpbbLEV3dobpo1GqeEE2pnqfr2afr4idTlEnvjd1gMsPGq38dXe4GX1P5Iy56AkLZTh8Lwd2D5S0Tbi5tU6-Wveji8z1-7TbTkrzJYSb3bUkrTHzVT0oIK4lB-80U1FJBDenleXkzFdEbpC16DHCMOsZ0kfnbj_2ZtQVZwujXk4S_H4yn_a-8Tb-ehQosrRtuDSx9REB-zgQHg=w868-h626-no)\n",
    "\n",
    "The next outlier in this dataset ends up being legitimate -- it is a Wal Mart that generates quite a few calls. Sometimes you want to make sure the outliers though in this graph are not some default address that is often used for incidents. One example is the local police department -- you shouldn't consider those walk in calls to count towards service areas. But here you can see that whether you just use the simple count of calls or the weighted number of calls does not make a big difference for this dataset, as the two measures are highly correlated. \n",
    "\n",
    "The last thing we want to do is to join this aggregated dataset into the reporting areas shapefile. In the table of contents, right click on the ReportingAreas polygons, and select Joins and Relates, and then Join. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/jzxSwqy4LeIGuOdAMcBvbS1cIoU12yrCCHsb6ZvWYXhqFrJkQD9FdvPpAdeQiUOyHthRYM69-Sx_QzqFgPie0a7WXOhjCuOgzBJ81TH841UKVWE8e36p9DbklOWsC4hwhLYs6YVJfvh7C89hytj9UGKDFeKcFsrI70EmHTCMKLupWb2F4PRJC-ubvE7mOtyRhFBSRS2wJoYqjBuMYY0PO0q_u37HNnMM8-NSajiVzcYWnzghtr6j5Cqhhdsy3FJulNHeBmnyoI0s5cx27C78pTJSvpKUEn7ORaEk68ruE3ME4cEmheL859RNyek2C45NgCwaC0v099Dp_AT9qDVxP7QO3vl7O2mmPRdoJqvWrS8aBxe2WX7P1etHZMl1qEiSgAfSzk0SVyCCmzZZxNbBd5l4C6BARq_0P7oPie9uI1Xq0M16wJpHqFJQv2LPC3v9TSiSfInye-DdbEtHpzJCLg8MA4rtY85hPmD6GEhyln-l4aU7zjXHZz7Bujsl77lEOoAsDCevFKtcxUoHzfBhgmC82xbSqpMqMWnM_BroNs684BggC628zgBZPf12WUqm6N7oPoIA3VyUG3-BqUwIEGcEIBx6weTn_VAZccy4Ug=w816-h616-no)\n",
    "\n",
    "In the dialog that pops up, select the option Join attributes from a table, select the aggregated RA_Calls table, and then select PDGrid and ra as the fields to join together. After you have filled out all those fields, select OK.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/brinAXAd6A7TCgawr4uKlzKFj8kDbKzXHg5dRWx-4VIsCv3vKFG0uk879ehjiToZ8hA3PVNT5qStwJAWtR1gozMeLht8LVxcZE3UI0nPwbSaD4e7NOQ75qE3e2ZSUXnY7eJD7si61Szu2D17ACKM6JuRWs8mqiDkAn3jRr02A4JAdlVSqpo7BHMr1sQminSl1rEYQoOsgHL1cnpt3IxtOZ9DF-ExlzjR1iPX9ClwPV_sW7K3vsLE1XawSsWnd_eGRy_LZSZKE5WYOoAkEJs91j9cDDfJgIdXi4jsfFNfWvvqxFvQkSlIWpR_1SzRFpnfuJKtgOJju8Ay5ELg5Bh3uIFdMD24YDdQ8a5CVTlXfIMJvbxr74gb_oRNFdeyn0GCVxDt7cui1ShgLGX3dluWWd7Mc98gZI4iQl0_sjpQFi-IIFuA2Z__qGutj5ru45UD_ZnJ6rEoryjtTtSMlDS0R7f0AJTA5sZsc7DgsmnOhFvQx-q0SOlNWJW7_6bQ5pRY2hwgDfjXhUMWsfSZONdvHbAhgJe8zzm00LJiIMMAe376wC549Y8V3ZA9m49eI6mzCWWBUupkW71pAZkIBYwFlL3miYAsYCorfpH3gGtulg=w412-h606-no)\n",
    "\n",
    "Now go ahead and open up the reporting areas attribute table. You should see the call fields now added into the database. Keep this attribute table open for the next step -- calculating the travel time between reporting areas.\n",
    "\n",
    "## Calculating distance on the street network\n",
    "\n",
    "To calculate the time traveled in between each reporting area, we will be simply using the centroid of the reporting area, and then calculating the travel time in between all of the centroids. To do this we need to do several steps. First we will create the points that represent the centroids of the areas. Then we will create our network dataset, and finally we will then calculate an origin-destination (OD) matrix for the time in takes in minutes to travel from one reporting area centroid to all of the other reporting area centroids. (For a good supplementary resource on learning how to use the Network Analyst extension in ArcGIS, consult the GIS Tutorial 2 book by David Allen.)\n",
    "\n",
    "### Creating A Centroid Layer from Reporting Areas\n",
    "\n",
    "First, open the reporting areas table, then in the list icon in the top left, select Add Field. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/fQ3aCd7oG9aVoxcl5mETjK4VvJvQwIChc7q1_qTa4asMAZ5gOpuaI96Ytpg6WI5DoATlaYSGuw5IrfXJ2_B6artDkaVW67YGXa9nss15brC647xrczVDdvSGyJwZsdLrCyQmZoht-ixyo9_c4aFi_fWn3C45i0TFtV7OFLdhNKFLi3yhHeuKFMRM5Q0J0MbKYRh-OXfyqZtv6inn9NxMezxRtEBVTpYE8cm5sdGv5lLDb-Py_MZzDRkRTLdbdk6ds1JMUEGH2IWWC4PG0GWD2pudJk5lVmNSaENM7XRmx0UaMPpKoZNw6N8EzglKYaRUh32xztAKk4Qevtw84lYpbLXzwkWKDw9TfiEi4-B05mySyUuOvpQVmQwPnqHxaIFwaiJmcZHFEbH-38PKZTVxOQOFtlkyCmkuh8pirryLYWcE_aBBp8R1MHdHRv9wzieTnWVKOmraL-KYxtfHpbRVVcF-fmws2pWAmlNZps12mwRDaZ-A8Z3z3u2M_E_aRlL7ie29dX_SY5dozQ_fPMv8P-BoyUrl_CKLkGQRMXNKJR2yT7TLuRTyodOZEbxkyhM8YBWb70jm9DSR8MAcjROASen0a7FO-q0gA5YKpn7fNA=w348-h524-no)\n",
    "\n",
    "Name this field XFeet, and select the data type as Double, then click OK.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/AkKFOtY7xn_vKW_CpkQR-zuYpnwnHASihF-0OjTxmVhczTlJRkC5CGt3zuLIaZJsGa4zFDC4tY0NqEZgYGxxTzvRDiCUSLAbSHqY2sF6e19r83x_V_dkFz1jqEWoQOqeMKtBsxjhsW0-G-aOWBV0FESkh96JDiXZZZAo7LLSqAdxP9ARBMZEhHA5I3iRdLC8nNtiyMmWi54Rshu9NbgcpfRZutkmWsBj3mun7caefq3wrAJ0DTTxbD_rAr3ECQ4p9zvYnaVMzuD8V-J7ddTEZS0kgbC9Br2Dk6y8xZYHphgHAwGMdkq-uD61I0EsMWsn2ahY0XdYbmOaOJAiK6e40uOu6p21xd1bzyofXcBWOjINEWuyWevabhyM6CTcP8bcoKWnZ6RuW0yP5eBR6ymbZbljhCeHAcc-gHtxbdVQoAhmSedNgVSHKxiDEp_SpRoJ0JhDUT2yPRMLlrHgy61oTy3JcJAJqtcnVnKPQ_50clbgChiTuRZzo80g2cNzIh6Tzb8dRn36pb-kXy_0uSD5V8Pd2Lx51nGakhR0MqUo0AF1JbXTpM6MNn6sNRlG570Ops4bMhvZsdvNpo95B_paahqtZJ_wJviW2brw2MQjRQ=w296-h304-no)\n",
    "\n",
    "If you scroll to the right of the table, you will now see the new variable you just created, but with all zeroes. Right click on that column and select Calculate Geometry.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/jW5f_XYwLugNqUP6Fv-V62ZWEl2IyxF7LNBCgwbSY5HH8zXYEzGIaj1gezBs2eaaOAmj9JZLh8RHCPmnqoLP_CmusRxl4nHZgaMWPRKuFOBe7uWmkIfvu5yOsi5xvJl_xd8ARMZxfBtJ9ZeWL4EcPAPdDnJQzd1RdGfiHSLpjbMmGspMMYwAh-73V3X7nH-iJYBe3vciyTB2gJpjrUumKcf8Df_GSIkTZF6MVL5yV3FH1eEcEpjOhn7eKeOJ6xHY36p6tkZ-AUZTCFzV81_JZTNqJjbtVD7ecp6In0t9BF0ryKyf0ryr0-kDIXGI-GbngOHlC2x4kYABs5oR1404_AgrMeH5DdZaFYowWrH8HI1CuFeW_5nj9r6sTqWFCYHtfRqc65xld1aJ_cs-bw4_NTD-83siaqtgARQ0NswDKIUu1d8oxHLoWua5llUbxvA5EhJ6iCf-veXwyUKrMu71i9rmvWpWAGWaYJsLZKYc2qZklGFZGyD_Q5pjsycXHbHXRvZKqNm4npN8IG23-kSlDTNHMxaIDMSkWR1S6IcsKgRkO3DSFehFaLiHxuHnOtpd_jOojLNdtBv8h-d4SVnv6kbQ4Yt_4OStOtI6IuUKYQ=w588-h508-no)\n",
    "\n",
    "Now in the dialog select X coordinate of centroid, and then select Feet. Then press OK.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/EK96G_lKN3_Wcbe_KxbTcIshY1VuJHSpvHUqpVQHEmzwZg6-mbzbO7RXiD5HdvJTjVRwp66aSS0Ldnl-ck9LDeCwgx5zbmoG8cZ2LXeV81zEp7tyKJg61yYVCvCwZMVvs5WxCp7ZGfLiiBA6bK12RMI6vAjn7ntaU2D_Vvg7N8t6zWjc36_VBjzlgyHsXYh87NAsuFmaGO-UQUbKPQxhIHIiqbNuyZdaEaj77AsNBRUtIgkGPzgBUTwVvlkI3stubZOIZcePjRgXrZI60iq0_3Rje0yYMGESuzgqTGj1OrEhxRN4N64txc7lfSO048HeznUaikG513PDQNCnAnsICGS2vC2cf2mJ-J6zOg5v562LE8vw0xx14dIDm39fOrl8cmMk8CL-JE_p1ndwDcK1ZhKyIhsJgC2JLsgfQQIQ9wMk2-znMp68f5wyXdVtuvm8OuBM9zPSD0ImhB-4Dofo0y_RCdBfP8YgLfPIBNGZ9brbOTqSbZPLATsz3nHRYFOImAvOvAjk8KBCGie0JsunD11YJgfPIRXJn3Xa9Hr3T_35Zw8oToUOiapHs5_UHKChfcynNKyMJt5EhY9CP8olhLB2gQtGX9JFTgYrf3EaTw=w452-h328-no)\n",
    "\n",
    "This gives you X coordinate of the centroid. Repeat the same steps, but make a new variable called YFeet to calculate the Y coordinate of the centroid. Once that is done, your table should look something like below (your reporting areas may not be sorted in the same order as mine though -- mine are sorted by the PDGrid variable.)\n",
    "\n",
    "![](https://lh3.googleusercontent.com/2XSGWwC-_nltjUy46LST3vfoubPgb2kbhmP10UWt7186X1IHF-6chjxhFWH_eO1eOcWK-1KsgGnhgKXIi3OBgDBDd-vDRf5kAFXbD0EFNEUMltqVoMIUWLpC2nXIFvk4uyT59H4el_oLEXHPB5j5p3fbI5EQrpSnEK2fVgp3dolLTdj4zvqe3n3cdjSQLg0N2qCeDhZ9aUKPMWZ41zK7H5bf-oEBflLGfcjafwP-yyz-4rRjdmBn_pbZKEbh_sDTK00ba_ToFjhMpApGgFEnYQHgP8jsNkOOttEI3WN_gKIyq1keylHR9lYgIeyFps3go2Jxcdt1InzJFVYvQn25pjdlnzJM_N3tp3oUOedA9028IPIrW2gtXXkNuL2DtXOnHVbLxg3BO2mdIE8KmwIIAuFSD8SebnXiXLSbJCI8PR7LuXILesHkip2W5VDo6Q49ZVLk-4E_3tNoBLe6aQf_4Posv3K0reFxIjfOFNGvVgl4ZAHCaoSypszc1yQyMkk-10GnLfMNDAwhtL_lT4UI67_NezsdiRmdPjDuR4MDeYm2dDsZHjVEA5dOv1eEbPb48_vbqx7vf4Vp9GqjF17F6Jh2fkcSmwwDjp2x_agPSw=w536-h532-no)\n",
    "\n",
    "Now we are going to export this to a new table. Again click on the list icon in the top left, and select Export.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/BtaOn1qquGtLU4nWDuIA7e-X00U9Q_b1JSohgxSE8EXcPUYPlCWy9j0NngJ2ZaSyUHnn4Jdq6KiKpCCMseZKDbDFT5grsKGKUCwDIP7IN6HXDCCfIUDf33JHpex42tutz4dE-mOWqFI3fMx07B96QomoOSlkUsfdswiev6A0pLrjpxV69iz_wtorVHk6_HIqpE0hj317UVwz9BK-3PY539Fi3ukoJIlALBI5OjljJmpOYBFsw3z2kNp4D5TZpeGsHU7d2ooL4i2UiMeOHtKa7vQ7mZ0Y7GrFAou_Bf9cw3wnMA9Upbsq2KwYWiQgOR-NrjQKFD7HbzcvnjeihStVmoTcAzcGw8v5x0AudhflP0yqZ1lntxx-tbpseUDNSqz20u92HxxGN2WFwVno2w5YcNG97hVH_GI1Y5Tkc19GuVipUhrBpA2-yiK4JNQhxSPB7ngtfeZTR6d5SmUO-Pu6AC3byjO2gJNR2_-_nDVyu0APusg4nkVyuN6ugdVIFTzsSJloJVlkMPuupm3VK0d7csz2fa2GqaeG-viP613N9s9vNZDM6AiSpu1W7V-of0zgAWjqfNEMYpNxUG5u_HJFKFbm3ovTrPO7fr_9FXLK-w=w360-h572-no)\n",
    "\n",
    "Here I am going to name this table `RA_Centroids.dbf` (to save as a DBF file, make sure that the Save As type: option is dBASE table). After clicking ok to save the file, click Yes to add the data into ArcGIS.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/Mvo32SHPsl9cw2UCYVcYPxEbrMAiKI-5fIzUan2-ABP-DZ6VQnVuJyznq3Kq-sepoLW_AiZP7pBeWxdfLoOPVq3zQA_q1WZxOIemgfyl8RsaVTTC03LmX0JhvdsMaaM5N-3vIO6xPhe-a_rQMVRdcRxi8mTrVcGl_Wo-c2sFD1RXK5HJoP1yJrzTK3QSk9OAtphXcJIPtr4iqItUq1TL4Sk_qxKPu6xIK3buoNDziRmJ1et9JKclq1DcXUY8Vy48ClB7GATpNfB71bFMvW4iLm5DSVdLz96JzdM7AhNFiyyVk3woiJ4m8Tf24ZwWXxZpFx3Es3A_2JYElzXUD8SDnUNJV169gYQh3dwl7GNgJLTl5ZBZO3U5Ca33foHVF5qqd_zdYNoPREP3MP2nPI_gwyYkkbLXzZReTxKwKJCcvKBWuoasOUvsV-QW-5wcHjCFxWlRVaM4-0aZMFprwnmgAZ08QxJMLdTJSZLf-fP-TUE1cx7xMkvWYKl_DQ_o9g3PX7XlQ4o4-pvMZxml_Ktm-oQWfBrlEil1IrJH5tQrS-aR6eW_r8IcvAfjG5-W6YEBDp06_TUg_34QU68j408FDbRKbgQ_loaw9pxiEwmFTw=w524-h356-no)\n",
    "\n",
    "Now right click on the RA_Centroids table we just created, and select Display XY data.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/-q2JxX1mTMwi26l-MJ8SEFDIda79_OwYlODyh4qn1xvEvQdv-hJNMRUgNyQpmdgyaYlWH3xiwtN6mN9zuAXCHkKD1rnbDiSgEY6nAGOT6RnUveMyGEOBY9LwgUSobF0crisNnnzNJM_w0c5DFCH8SgDqAZI6kiPO4pw1ole4GhNDo_DY8lWA7SHYqBuIO4oEdf9EaYsDauzJCeKTeGexLWJV4RqP-R0UW3tyX4vwAHqFSzkVGeoCNPnb7E4hJtVp8UBvv8IoARVs0xemSAN4vmBJI5VR3yfFO5jBaPYHDc6FNxf78LLLuYs6Wz4CF2wU1RwUJyMzpSNC8G3kckYXZwWLh8t8swYK6bku8XUMi8Z-LUOqCehGqchUwd-nF9_PVCqV8O1rqJUe4I8bZ58FEQFNE8Ey_oLZhOSuF00Y38GpVi-Cep0xP0HKvikUSXDmSsEasHehTwo62ZkOaGBeew-Fsxa2wvffylK-ukbwE7FbyaOLuIXJQDkotI_Cj4tgANuqqXRhNKiaJcADCu8byumtAZ1Hhp--me2jW8Z3BaY4v1_jTJzoBHVKGtq8gltRVhoA7NL6uAzXNrAyYIrp_rEa3DuqVa2pMluJX8x5nw=w536-h528-no)\n",
    "\n",
    "All of the options should be auto-populated how we want them here. You should plot the XFeet and YFeet fields, and the projection should be Texas North Central. So go ahead and click OK.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/Eh1A_zPF_WsSNgtVwPF-Q9xFJlRC_ZRf9h3eiQ8jwT_0qoX_Kn2U2jEC4D7cfGhv1ulAcXpcd1IZJB5jA8qd7-QlUeHOGsCtlF9PMZKZ2NKGf0OyiOmxcGNL9hzleZkHMfxFMho-RHXcwnjI4Xkjd_lWfFDA1RC0Zr7GFlhBXwsjsfrK9OiVPj9M1KOs6fXpKODsXAViFjyk6F6-E7Wt-jew93SiUpD2yauEQK7WHpUPhHIABAGT6BQIL2KWNVrhGEAWs8giTZr4zdbU_cgeUGjEzmSEbjqdlbXfRn-42darUMT68utRbrT0oDfHGQOI8DniJfuNo01eSJ5F17QDXk33yzVSEbibuy1OGn1IZk8jUJqCqgqbTdEI31vhJX_FzEGVwbpbZsrqRxVeNjEjiVLxYNUVWKCRSIxHQ9taimcXwgQqjokf7Er7_Go75GdvKizTjyXPCPbK6wGlt1G2-ZUfTJ-Px5blfaSDOVtHfD7BLAQuiTsek7RPf76VGhgVGsVqgvBnDRV8Aqgw0IebJSyyiFm-LqRGSgu4UuMzsAAkkgPBL24aewmGo24FcPWx5uWfh-vGUXFFwNqQwCkdYcE77fw8JvJawzr-TNsowA=w352-h556-no)\n",
    "\n",
    "You will then get a new layer that is the centroid for all of the polygons. \n",
    "\n",
    "### Creating a Network Dataset\n",
    "\n",
    "Now we will be creating the necessary network dataset. First, I will note that I have created a variable in the street centerline file named 'Minutes'. We will be using that variable to calculate the travel time in minutes. The function to create that variable is `Length/((\"Miles per Hour\"*5280)/60)`. Where length is the length of the street segment, and miles per hour is the speed limit of the road. Here I already calculated this variable, as there was some missing data in the roads fields in this centerline dataset (for roads that were not maintained by Carrollton). By naming this field Minutes it is automatically interpreted as a time variable when calculating the network distances.\n",
    "\n",
    "So first in ArcGIS open the ArcCatalog section by clicking on the small yellow filing cabinet icon on the toolbar. Below is the location for that icon on my machine, but yours may be in a different location. (If you cannot find that icon, open the Windows option in the main menu bar, you can click on the Catalog there.)\n",
    "\n",
    "![](https://lh3.googleusercontent.com/vNg2PmF1OYFEtQN1pNNdMdQX5PCgZPBsgqavNeNNfED2zNqpiM0Sq6K6AJMvVCBwclaGJEmwWd67m4vC1n5o3H_IOvpdNhgsYk98-92fbdkquerAAqb4GM-nPNX3OF4G0rg95MHdOkt-lceUHjz6yz0wKBHrAidkZ2Hh6GvBdJePDrogLosOFKLKLwlLA7Wq-7EWZ6retjY8SRZd6XAWCm3W9LM4GkFXrVROFR0wdFRmMYVqYq-mrhZbKVwGX1AmSg9feGpKRbwuiER97vqEPf5XYzdZipbdQDDdfYf9_Mx-NxtxV6FzSPfHoROxS-lur3QGvDofAw33GBlfvmPIMow7FqC-C8Wq7ZW42FTidFpbnMZYDTPv2_JvuRpW7yAEhq-CEPJW0YmduatjyVfL6aB70QeM4M-e95rNt9xteVPfc_TLacM63_aObA1grNowH33IAVdWgRciCiDG4F9RDe761q1ME_5JmlCnmiVQIjEyf8hO77whonC8lJ_Q4T4P0xDUtDk-MOpimzjyNxxC_8wweTMqNxZXDrMo4HAUUXCI6mEiIVcuibXliT-zd8xewRJUNsO_XnTwMGpJpJwufCz81O2PAsFT95dGKDUFYA=w519-h715-no)\n",
    "\n",
    "Navigate the file structure until you can find where your centerline file is located at. Then right click on the centerline file, and select New Network Dataset...\n",
    "\n",
    "![](https://lh3.googleusercontent.com/bFtQkRoeSGsJzbNcnGjWMn5Kn3ZblvfJK8_TVUMRUqRJZVEgP9JOI6mRaRBSPj7ex2sy8zmRNQzAleYMUa3CzhxaPPD0MchI9ElmZmw4VJ5YEwO0Pa3_nJ-ntl-o4pEANNp9dk734EfdGXRN9RNPK0C4Js_bis6WKYU4OPv8kZXVufgLrBpjePGnO3LQy4dSIuKFzDWleGoW_hNpTtO2flCDq6R9jI7L4i-1r_EiP4HE0x5ljcqPSBtYZBZb1CdJjDTsDOayLge_dhSDojzE7LkxC7svRgV64jFieVuGxhD1f6PRvRguSD7QkxMN64Jb6SuwWBHqaMIfsWuNwwfly4W5EpggDdXPQPPCk1gmDzilBKOKlT5WrYsh-zScUwS1XSXGr5Jg3wRIpOKgYbaTuOYySb_zsQ9cMKQUwvv-42VGzPdZCTn_LsSqWsDuYBvBtILiO3xCttyLMd_buXOPs2eQk4ZSdlgVcGNcUa_OP-gkF2-sFZCR5M4jeiwUIpUkSr-K1YYU81CiqM6aWKbSiUOtvS89rxlBp9GMlu2Qomf2BMfWQan7j40jKxfJYzD7ZpVF9k1IxGtdOdnS78ZNbwaSweXbtlzErnfiazO4dg=w458-h368-no)\n",
    "\n",
    "The next several screens will walk you through creating the network dataset. We can keep all of the defaults here, so click Next 7 times. Your end Network will generate a report like below, and you can go ahead and click Finish then.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/seyKhazywTmJgcQPhEFnjHbu_kxseU2F8q2mEIsCZ-4smXP03cj4fuzkWE28n887m_OAcRRlDUJkaEUY6lCLNr_B_OS0rj31-ecdSDIi74Weur2bZdCp63sCo3B6l-3684VyEEY_dfmGdGya8ZE5PV6hCKuDPxREkrGjJWGdV17VVyVJ3a3eFCOwfTJGCxo9xkaOcBYWRqiL-x0B2oqBXNQQBiyT2V47VF0TpAjHgWREbZUvlpzk4KqWZC1s55MrpqUcf27RlG8l_LRL8wVNJREBEIGdlQevhGRQQDTSZYN47aBiPAwe29_ANz04lYOiD3B749FmjkBp0uCRgWz-AqPyTXX0ek4E53LEMCy2F3uyYj7AoDa93cJAGp3EnqSwRJmsubDg_sEHGZ8Vuc0TA_j1PFTqSsUsaUNGsri2a0lZOr1Q7Zxr6XXD_F0gXV2WAtwsKPtsDh_sJtLkXviSs72PlRGgu3gzwTcdxrat-HJxjx4QFmJJ7sZWuYoaOPKmsp8NT2VJmEJtxEJN6GQ9HutFoTaFrZs_RyAXMktZYYBuegjgYeb7D7tIKSZQsU3M4tVdn9ZFGJnePejAEzqae8qrPqxY3tkEm-SaiRHW0g=w728-h682-no)\n",
    "\n",
    "After you click Finish, you will get an option to build the network dataset, go ahead and click Yes.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/wF7GHsbpiEji1pZ_GIgsNoPox2_hw3eNfOHLqHMFE3jo_bMjLd6YpwEH2uXNs6gnaC0GVRnfXj_cB2j4LTOrDxj7lgQTf_XFN35GHAnVhu8o3IZSKmAolc610p1M2GpCaSEw-R1-td0ll3n-zakeK2tT3OuWITdaFdrF9-iPnZoUi4p9R63wGmKmsZEld1H4slAylsjr5pIL0VpuM2rJloXYx7Zqzu-TEj492LjxZ8yMOSgrEjgCmglDhSCMVgjPN3g0_ZrgKIvEWwgJLNohkusaW2K0WorZGT1smAzikANNWcAm7Y_-IhcLQQLDaMZ7OD_oi-iamkK97cPnbOvdtEZ0ga0ihpvKwGqgjgM4LPAzf1J0vSjluQ9FAdjSdyVJ5T_wjsnmNgB6zhbAQl-XP1vePwaDrO1JU6qaTBVHMDDPWTE7KDIhldGtahsUmCbTCsOQJ0nh9S7PH5a8SKGDQ4Xfl6TrjxIreLdDbiXpOCl-n0kPutevXkXiZVasrsBog7Okaq9PfAGdy8SR9dEqgxq46OBl91mrflrIp9o2Fau-HKNzeYzjdn2IvQyeuoWaCdwKqhds-CJheWyIC0j50iJGfoGGCrW-izGuH50tcw=w447-h143-no)\n",
    "\n",
    "You will then be asked to add the features to the map, which you can say no to. After that, go ahead and close the ArcCatalog section. Now we will be adding the Network Analyst toolbar to ArcGIS. In the main menu bars, select Customize -> Toolbars -> Network Analyst.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/QMLWsev-PomnZsxJ2XpuBsIZ_ehbVJbYtEyBRr6UBvaa5V5AYqULzFpm2DE5BkjIXcy7GKMETBgFcv8NTl5-7TiJ16jDDDy1PrSZ6ec08Wffaw6bUZ7vD8LVc32rpbcaAiULBLJ55WMKLlSZoGJkgWsjNK2Moc-S0RvmytvSzpjXg6yaKqQz5Le3VQiPv_cA6pqKe8USTevp0ru69SrSJ9HvnC9zlkni99v5dn64kVmpnhjYpIEyckuLDBg1YuLBkPRgsMnAn4jSFJh9D-FKfqGMakJtefzQUiPmxiMYdNEa-r7Tv9XCIyGx08bKQ-sPpz4s6rf5NB0jB5jwqo9zUEp2tc-i-mc27TIW7n0Zd_2FOxRb1LM71gQVvFnCS2Vndud_mV8hjMjjH6j4X2wVOI03YRTY8_KOofsY0MQlu-edcx2Qz3xm0_HTiCsMFFpvVsPj41oArU0AMx_iNERrDBraRwpoAnDd74usK7Y4VHLoZVjy7pjM3YIuJ72lHLtXqK52mkLGpMebwt-_rpmVvYI4RN9fjaOXWtmuNU7y1TnEDxayPVxsak70VFC-CdhUW2ROXORpDbBhxfgUbGr_8l-3ETkuLiVpRA_nBkFfJg=w452-h696-no)\n",
    "\n",
    "Then in the floating toolbar that pops up, select the Network Analyst dropdown on the left hand most side, and select New OD Cost matrix. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/rTz19kte3p420iuqjTyP1FtXWeOpKBcd9e-B8rzSLkxbwkh5_sc8Ghup3UagFrOwfl6DJqJ5hC0pnwLvXFa-kdIMz-HVVQGeerTAF8nZwWV6lw8hJfzxr25z1m4aSjTqRY-Btk-XgJ0FFDy6icj2V-HAtA99SYa5R6qNe7Bd3s3TK8nXgY4ZIUHZL6deJkaw4I8p48_r5GJQj7J3WXYpbdmNJLOD4J5kkt29QSrtzcy9Jn1bNJBzFgqm_WHLss4XeZezb-l-cW0mObLnp-Drgxkj6LVdkoxL0NakNFhWonoeO3wC-IQwYsdsrxo9sHxu_X0G9JfRc3ylV8o5aHm1Wug5d-oRVASL33PYyQTOPFc_jqBqhn86ps3XeItdDSjD_Y2b7QcojeCnWi46Ips0pg_ngHk_IuxByP-udPQaICPvJ-hHYUw6w1nE2-sWCAOouQCOKoVe4UhdmZmP99xOja76bH5w3DQDBm_Gi6fH7FGqHSZQ4381hl3I-TzMsYYm0sjAV3eimkbSBwBOaUvIl-e1ueWhGKSObiEV7tu4RMTvRJ4MNOXhdTE30P-qa15WxEN_qoPryF3GHBObXSrT-y7s5eKp6YaUMgRvpuUE6A=w538-h496-no)\n",
    "\n",
    "This will add a set of new items into the table of contents, and we need to input data into the origins and destinations to calculate the travel time distance. In the main menu select Windows -> Search.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/Dz_KqDh2PJ5xa6DdX2I1_BKWTXlBrl11EP9cQs3gzS0tH1xSi61C1WBWqZ9Mf_dfTNXcENJZCaIYPmIvJgiIBwFskOZbowxaRebxE41o9txY6Kid0zR3LPlMS6XROho96UFI0VEod3q7pquUN4FZzTc44U58we5-lBzDFMugKZMqN3BeYMuWOiMnTYDJ_sJwcu5yo1Ptqj7ap1W_LCW3DIyYIxito_yQdYwKpnZZLO9mjqlb2uCtUsw2a9eraqJdjApn-rrnDu0WuMD4RxWDpdG2AGPqrjpt4jUWvDoL3cMnYjbN0xy0Q5pHlc9Qefzp9FkpE8zhRWogQ3Ta-HyIROW9Jyi7P1iJ_hyq-HdCV-EjDlC8UBe5i5r42ueVy8DvCoCpJMOwEnWPa40PWQe2_eLJgyZxQPPwY4XouE-ukVJ8HdnqtbFRsE_J07K3OVSTR-zpexUZSWQj9TvYRowdxcEurbYQREdW4vdV11uEmd1IJlNbZm095W6zxlZwiii5zNECM_bLv8wU224bClrUTC-WqZbSK57aeXm82b_Mr0IpzTl-ey732nXK3RoW86eTQMxOD-03wXAXf1OBlg0YpI9ft4e5SaBrA35xn1dnuQ=w834-h576-no)\n",
    "\n",
    "In the search bar then type \"Add Locations\" and then hit enter. You should get an option to Add Locations (Network Analyst).  \n",
    "\n",
    "![](https://lh3.googleusercontent.com/-b-6xZRSb5QPgc1HE5BDO3OP6TVmCXEcy6KJnfB5x__lIc-l1SOuUYm6f7OJThkYPc6kJgv-jDBBk_JL2XX5iAySjH9GIGud6W81lUe6th-ZaZ1vHjBA2WCCKfLr8gy8f2AGtLa0uDZesWo2mx0BT9G-1Yom8kuaPFPK1j3TLBMu8fVCT3oBqrMhNnoVRh2hnakT_zVpQefgoBUDDOOd4v9RYQ1qJ9VvnVJ5GZZJjJxlwPL9yZyJaCjivkXnvYrXUHtJJ-MPL0Jha-G3ov2BMAf5ujPwEra9wZdzcaMaz8vXWwoKbzBnMlHyw67E8FWd1ZhI8hRBo7N1rUBP7-JwV8s1n8QPvgw0ikqNAdFoD88kmM64FfP4s_N1BvlK7pMtXp799194k6w2VKufDe2fxK8T8T7rc6-NBw-C_Od5--0HAb4elXu2yWP2juDxKtRSU-VBz9cpsWL8jAjPBlGjYD4IDM6HmiS4kGMq9Pr1SPwGF10_pF45vpNSIs6BcqxcAKX0PHwu0zk8aqTC18_J1wTxTKCeYSacDrvHYR3zQUZoQ9RsS-wmURX6eMmMnaOKF6uOPkESkxTH4ZI4hPsxs0omdpy46G9YVALMyohFYQ=w418-h562-no)\n",
    "\n",
    "Click on that tool. Now we are going to add in the RA Centroids we created as both the origins and the destinations. So here we have a few sets of items to fill in:\n",
    "\n",
    " - For the input network analysis layer select OD Cost Matrix\n",
    " - Sublayer - origins\n",
    " - Input Locations RA_Centroids Events\n",
    " - For field mappings select PDGrid as the Name, others leave as default\n",
    " - For the options you can deselect Find Closet among all classes and Append to Existing Locations, and select Snap to Network. \n",
    "\n",
    "I have highlighted in red all of these options. The rest of the items in the dialog are left as the default.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/Ui5laaTk1-yyHQO8EFCTWC5Q2O3VuGZ8YbCxXqkvFi7tgGrnhCEPzT2U0YCXRBm6oobkoqpnUd-5XRFFuJrKU3E2X_R7XDHHpBUBtheQ4IeHe8z2z_orgZsTZ4ByuzVxO0xoY70NTToR29bufIonQwS_Gaomz9hNYyYFr6qluBdDUdpffE1X0C9sS7mYHbqGpOsdMVsUePqphRMeFJ6qNaC5A41fSLfzQhtpsHNqidhJ3A5uagAYH4P1C9jJE2QEVRL77FCJqpqBW_pF_4N5JXxyfO_eNagCP4lWsMoRBjjqaq_IBQCJk_X3x0NYH_5fnZgChlSVsmZxJ39ZzVL8VubgeufTVW1B1ViQzF54s9C-XeEbzCsQNCQKn_nUCLjYr3nWtZv5ZWXv8MtIHQ9Z-Z0U0UslXkET49gzBEe45pa0rQ6_Pii1RYT3GsQfnVU7lX-G5cc2g09C4Zb3iba6A0QmfYFlagnVOE7qFcsUow21hv6I6RdUTamQ_N7URDjcGrF7uARPIWeWtrJErKeeSyNY4i-YPPCdr9AEGTJBLhf3oWuVJz1yrrjao4CvVhJzMOKzPDwT413XMsfBqRKLbn81aO8lvHhZTsfwNG3SBw=w1067-h717-no)\n",
    "\n",
    "Once you have all those options specified, go ahead and click OK. It takes about a minute to add those locations, and then you will see a set of points pop up on the map. Because we snapped to the network, the points will be nearby the roads network now. We are now going to do the same thing, but add in the destinations. Go ahead and click the Add Locations tool again. Now in the sublayer option select Destinations, but otherwise fill out the fields the same way. Again for name select the PDGrid. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/DRfjBHuzZPFqXdUn625TSgEAup39s9SyJKGihRoF4XoNNQ2waFFv0-3VTDgbc6uQiDsTzaeQ_mcfA2EcEwO2EirJ4f4ZjjNtLRQJAi-KWVQAUA-V53_u08-4gZKJradjBdqaqEco-aJ1QVbB02aC8sRJmgCDG2C4soOVf6ggC2O15e-5OTMFmXwjc82XchrCChGXYAj5YCKHk0LVokH-GT_QIypnIGowIyJQsPu3CYTP-In-WfuRaRVOyj7ZlEqx-apzDO9Tgficw-owwicraRH4fwGsflPojOvLoew_nIvz2dIGcGRUAL1kpB5iXDB5HoRuRXeGiCiMZAZwUQhpx2-1vroyGEpdDuNPIVfoBavhqB4nYz_pwR-7jK6mgESo35mDDe53QZ5clMLe045k74iHtSH_xN9bqClOCSnbKHvkznMecxx0hVOxaUK8IOJwO1Pm_RXscEupoWLE4nrgpg6EuErnc2QviQ_LV8p12oMklN4mIB_B2DzMJAWp0LOplnJlKS2rvt6odf-x0JBNR-Y7Gz4bMPIiRTuP7oSCD0FPtPNeI_4Kqoiffb5oMhUrzeU3cGqQhNlB2u8uQ5VgqKQ0O_v3ZuoOgwz0Td58Kw=w1067-h717-no)\n",
    "\n",
    "Once those are filled out, go ahead and click OK. Again this takes about a minute, and once that is done you should see in your map a set of origin circles and destination squares, and they should overlap one another. Now in the network analyst toolbar select the icon that is a grid with the line across it.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/dqpUXKBIodP2WWChvBGXiCKDE1BF_iCOxxESCFscfuQrgDItnslE1Yf_OJ1siami581Ah9YSSHgCwoVoTHbmuxS8EQ7d_fsDx6xWdMFhvQHlblX-UBTmqR_qTNI82WBRfRccz2Jj8ucm7DAV6s35FAFSVe7E054BQfqXlPt-0SnpcfNpRh_eKgGDpAemufC6gnNn7iTGq1n5L7YHBKvVguOBeM9ID4s9rGIfF8rOP565mZo4AVcDlk5NTBqJb7TeReUDzvq2muek3dQAM5Aa4R5gQ5pOGprmUll66VTj_t_H1fQc2_BJvPuSWkQBwsj9lkqbFlnO3HnbMRLwf8gQZ25OKeOy-XQ3yUn--4h8eMMWDlbSBRS6Zz8AYK3mutpYm41KXGqju2X802Qa9PTR4_x8sAsyCrQsjxRCmkgAUYHzfyqnYnszK7PJ0BiljN47xcv9XV21K2qygX29G17IZX3k0S-kKYymlSqz2FJvnd0YQxB3NwcdrOfVXFXC1vvqV8nAJo08LioafmZwe8Nw8B_Js891G_v8mI9PN7iketKe4Qq6jjjAQ1VRftYaBtmOOaRPlDcSBnEsDsx32d_fPWFX5i7msVetynHUOvYZHA=w977-h841-no)\n",
    "\n",
    "That calculates your network routes for the origins and destinations. This runs quite fast, even with very many of both. Now for the lines layer in the OD Cost Matrix, right click and open up the attribute table. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/-kQAfkKrgXvcba8Ly4CettrkgiExWFoe9QWOFvujQkk6IgFCdm06Tz5AM7rY4vDQKg9X3MYDgAx---NlBB8QMhKz6OPicGDv3phc4Sw11wMQMg8f1AYcr_gvgOntHJAn-yn7uup2CfvqR5cTz_N0C12Ct2yJ3Zy-fq4EzViFqePBzCHz76Gaai-4SntpPTQH834dc6D21wNcxsKacu7WhxJaHAORtt9aIo6r0FssgYG0sMccnZETnbFHmbmK3_563fjEf90Z5JS8Z03AdGXJjfiw3k5lw1stpJ7Rr6HzsDKv_vsqDRaL-hdmRHXUVuAfYW4DZeM5PaWpaazrt1oo48ASYFzQ6tArK7ujNzl0y5Lf2na9SmslFdla9VV0DPv25f-SzGBjY-bgfMYELk0vnZuZ56rHlvs7BadkLqMPFNCLVEBfSJEk_NB2YprsL5nQQQj5Mlp4fRhd6spY98_yiOopk0CFmpMKltgeU6wn49GlRNzCq4d6A5lMiUd7Ewi1xkxa4n1uCwXyWRnybREOwPDNNoJkICywqLZJIFWpbMwj-QW-nKAOhYkiGhmqd1k-eqZm5QDlblJ_mR7dbVqq3_cXep7QhbidraOUDm2gQQ=w1212-h814-no)\n",
    "\n",
    "You will see that we have the origin-destination and the travel length and travel time in this attribute table. Go ahead and export this table, I ended up naming it `RA_DistMat.dbf`. You do not need to add this table to the current map. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/oBZdxddAon-FpI9qlxqS7UEqnx9zCQLqCToNNYjLd2rFXyFsayXMw1udPA-oFdepX-wTfatnanSifCWXxAEluELv453W5s_JKaxp1C9FZj6DwO1emxz50eCv3OSptE45z9BkLcuP2savpRr-mYQtUhnl4W_5fyBf1hHqkYgCHFcYgu7EwomCBylmgvfqPx3SriH8L_G6KSXITF9eKMDhzTEmrHA_tnjf83qi0QW4ZHhaed1wpnxGS_2YLDpTpZRpX8H9_J8aOuwsid1FX-emyc92JmSLL3l6d2jIaUtSMpGpOCReq0eC61o4_ZNXAWRslhKw4gIBh9gnm0aIs5tlbjji46OYlotwKUH8PEI_jdL6BDBGML3hFbIpuEZtFV-FV9JYJi81zJ034D9GVWxk4ldyat4rPduLUovQFnJtr_68xyB6CsifTnlbIYDOi6ejpFqlCr_gve7zj1S6NP-d_aCLqgkNIa_vLdUtAspJIr2EOUQxAvaBtxD1iVvT8yHWptJ8anVPH9mwKVpXr7SP1ALV6Mk6W9WEX67NTJTLjxTifYmLgpNx-qCMrB_EyQ06EZYJJX3XPOEmM1wj9DH9ZqS9U_7V_DatrrMLXtdpUg=w662-h626-no)\n",
    "\n",
    "Unfortunately, the network analysis tool was not able to find a route between all of the locations. With an original 325 reporting areas, there should be 325^2=105,625 items in the distance matrix. You can see on the map the RA location in the top right has no lines emanating to or from it. I will show in a following section how to impute travel times by using linear regression (they end up being highly correlated with Euclidean distances). \n",
    " \n",
    "Once you have exported the distance matrix table, you can go ahead and remove the OD Cost Matrix from the table of contents in the map. \n",
    " \n",
    "## Creating a contiguity matrix for the micro atoms\n",
    "\n",
    "The last step we need to undertake before we have all the data we need for python is to create a contiguity matrix for the micro atoms. In the search bar type \"Contiguity matrix\", you will get an option to generate a spatial weights matrix.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/4Bwg6odhXZMOZCte0HmVYP0iuGm8CbuB12amvnyn1NsSCkKNA7aoPdZHX0EOfs10uc2Zxfes6fFJxtHnC2mfI_Tf7LOsZw9lksL3rRQuEUT-_4s8wpVPj4s9Nc9yMHuI06Q5clfavZcxlzpAKWQqRx_Vc72QzhXr_CtHJ-ZKU_S7jPQ62p9P68zNo8Ce-o4oG5N8R2SiuZGARMBtcwfdV862DzJpXTFq6WjIU79hevaoVGlOPstDdgTkc8hRhStJ2cyFXLKaE8HmnvSLPM8HP9MHgkIB1k6ghzeJiS36s-2cpU7axc7Sg9nQAyevo2tE6pa0XBJquxNPtAv3R4Y6h92yRmbffdtLf5xhLf_hRDRLLFt3Wtm_cYXZrkJBIjiveqjv3-cHXJeU707_1yFlo19_rZ87JR33qiPouFlETpC8bnQVm_CzV-ES01ICxl22pp-d3z3a0WyQRznyorriTxpVogE2BONwQnV6fj4PldS5229JIRKH57mp9JkfnMu0MJ43eg4GuMeHa13DQzFdo2tLS9xjMpnxZ2iMAE89-nWVEGEIx4rDXwyERD9tdNCXMrLU2iyT7FDqwqcxjGP7LVP_qb-0xk8xorskpb92Qw=w393-h256-no)\n",
    "\n",
    "Go ahead and click that tool. Then in the following dialog, select the reporting areas shapefile as the input feature class. Then in the Unique ID field select the OBJECTID field (unfortunately this needs to be numeric, it does not let you choose a string field). Specify where to save the subsequent swm file, in the conceptualization of spatial relationships specify CONTIGUITY_EDGES_ONLY (e.g. Rooks contiguity), and then select off Row Standardization. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/5E1oiUyehPTncAfmJuxIskf4krOQyW5pWApvT0Ho5iCM61bhSvlhdl7CxhVXqMciAsuc6he1H5IPKSUh6WoP3UJIMk51NJHmC5FUfOxXaR0iz9FvRCvtPGswY8TTnDpxw0Djld3yc_OmatGSVIhV2bmedOi1WFU1q9qdnW5-hzhOFJPzoXC0EyV4TP9_FRnk7yWsfIoii7WQBCVXL8oedlOrol-PPOjrSKNBMtFUO6DgN2pEEg4pUd-na87Tt9LvqTlD08dXn8FNG0MDl6XijB1ec1FrHueR_7cFqTFJvYcG5Bj38rHjsCVen9gBPiC68TFjhGEOjnJLFYfRDdkxWJs1FHyJ0zZ5jsAP4kpwB4hJvZgNYU5VDXXKl5nouhlpKclj7Y1gvDIMVuhjg9usHPkAzo7ko4V19m7tOhx6SK9nQS_APsT-Xe3C7wrQfSZzu7BdvxhnEEwQiKG3etQVxIEE_O--yx_lEhQPZz_8D1OdeXXpwnLPYTZTSheDx7OofrEU84MtBgyUXgQoCJ5-zmxiPuH5gK16lPTylXnQd9yzj_hhI8G8mRmSykFQIkdJdNCqHpAn4Fd7bZqbyHApNKEVGw7JS4v5JX8TwXAYYw=w1067-h717-no)\n",
    "\n",
    "Once that is done (takes a minute), we need to turn the swm file into a table that we can read into python. In the search bar type \"convert swm\" and then click enter. You will get a script to convert the spatial weights matrix to table. Go ahead and click that tool.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/LPDQC0cO7Y7GJKvhNEZv3bfUw3AL8ggBpduhndIj28A8t6Szy4-AC6FdfPQt-mdxWkc_R8ZMMb14miYy3oX_-NLoBA_jaz6aCiexCAl9gGWl7SSSIj_Ky4GNj8Cd0IsBOpOtEmybWkWf7Y9ivUABmn2QIREH_jkI7qkdpArqD9S7xNWMQLnrKChzmxfv7jFQ8TV-roQmoKNBAzy41SycF1KlqAUyq297s3PVNjwfudwkfPAp4TRYgOMCCeHOYoyWKBq5noOI-jowfbC89lfUu4n2btGWfuRZU0ZldqEnKnp3Qw3RLraPJT5pAoMkHhTKcWByjgTMK4yoiIYdJl08oewvgojL6Y9gCdef_N1eTtqF6ltxdEodAD0eLr6Bgsn60Cd6mciX5HrjojlrDhLsjnwdcJJe6MB7DPygDIcgHS1R10Z-Aarvi-BcdSIB5U8G_Wt4zLwEXQZ75ASZt6iWyGpxSJoVm1SLkSEpYBwHbojM2_2tEYkGImKKIiACLdrSk0zknoQjcnfl6BihTkUO5IuRyw5K4XXe4sqXSU63I-z9Dd_e9e7cstpL1LBEJ6zf8Ouw4O5xLUq0iYcuuM1z9P33-W2KcFL6L0v8wv-lnw=w408-h266-no)\n",
    "\n",
    "In the input spatial weights matrix file select the swm file you just created. Here you need to save the output table into a database, so I keep the default database for Arc here. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/7IL3OBVN82ycR3dM0daaBzygwIijFXHb1WSKKjCoXdmmsBcLf9wH9dyQBwXfBVIw0s61L0NRcXY5r_xaJS23aMQ2SU-yUZ3KVs05jXNWEUPraBTMz-sLtgXIWoyiYIgE1uHKOz8axumH6K5E4yXBYMhcFnEU7X3i-WpLnrhKztYT1F54YXCce7jm-7suMDFY1CtkIzZOlwwMM4y9cOlVfo_LfRuJeyFzXZTdQndD8sqHa_jFqb4XzuzvPyGiYbCCQlhhZDK7D5TaM1ced8HKDWTUWWceWNLxerA1gJBQljbxqFGU9arZ73JTiQXFSw6PAqlfX7QjnZybMPYLWVZgdLbNyare8NFCpSGmKgeoGKr7II93A4s8baJI71boRyb3qFZVSQS4bMtzxYRPoi686qpoURA3lfOyAZywppsnJ6RiQX-ERgY-Xf-KDCpjF28MsGx0hduYAAs6N9rO6P9_ahhiDEJ9gR6DyzPZYjOfDrftDnYvMqsEcfVJZGj1kXk3IfhzQZA8RyC7Ba_xT8KAxLiBSNzen9A4yHIYDDlD52JGUT352QVOY0RZpN288mi7j9_YHNRFEVnUYTCo1SpCo7kFJu31S9SfW8MU_ofENA=w1067-h717-no)\n",
    "\n",
    "This automatically adds in the table into the map. If you notice the warnings, there is one area, 1333, that is listed as having no neighbors. This is clearly in error, and is due to some problem in the underlying geometry. It should have a neighbor of 1326. I will show in python how to correct this one omission. Now go ahead and export that table to a DBF file, save as the other tables. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/CGE7E__MwhPeDde7Rai8s5Z0vvSEKlWpymMBUBrMQCSOJ2bfKxVLFlqiNZ_2fQaskSnQy67sV4RmSI-O660s3b4Thf4a14jg7CkhlcWqOLjmrZXib1RX0pngPs9O_wwlN6VQ1kUsmaapEas5jC3DzwRFh6AJNlIXQbtY-DtHPybd48sJ1K2JwIGELLoquFgavzvbOF9hFZJAkqush9hZdHAAMa2NWi3MDnqI6v3srJp4tN8kezI0_dkFCx1B-S8IlbR-4a049PlTn-UX4i5SN9di3UH028Fy-W6jIS03YUl-w0qwuoFpM-CxGcCsK0DrTahmISIvtOrG5Q8mtRK6MuJjvbVV27zLyozGOGb7YVX4F0v029_TdEu9BQWRMdoWsspguFZm7V-mETMP2k2XG8oSKbhKhZDJapdgPZM0hezp3leFaC8HmgOHhu-00gfkaX4s5ZlSsyUF5rhbXtZFBCnLM4tEhcILdHCceAtK3q0TA4SY05QJtHiErKtQGmGoaw9ix-1fAaNIwS7LUAbdoqY5UX3_O57eIN6kkVrNz2Ro6XNLZwZg93B_15wHFUHobYZDDBXTVEbtJS5lO0VHttTsbMCUPLVpH2TvbLXITQ=w742-h366-no)\n",
    "\n",
    "Now we have all the necessary information to create our p-median model in python.\n",
    "\n",
    "## Importing the data into python\n",
    "\n",
    "Now we have all the necessary data to import into Python and estimate our P-median model. First we will import all of the necessary libraries we will be using (pulp, dbfread, pandas, and sklearn all need to be installed, the others should be available by default in Python). We will also change the working directory to where we have saved the resulting tables we needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apwhe\\Dropbox\\PublicCode_Git\\PatrolRedistrict\\PatrolRedistrict\\DataCreated\n"
     ]
    }
   ],
   "source": [
    "#This code imports the data and calculates the p-median model\n",
    "import pulp\n",
    "import dbfread\n",
    "import pandas\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import networkx\n",
    "from sklearn import linear_model\n",
    "    \n",
    "#Changing the directory\n",
    "MyLoc = r'C:\\Users\\apwhe\\Dropbox\\PublicCode_Git\\PatrolRedistrict\\PatrolRedistrict\\DataCreated'\n",
    "os.chdir(MyLoc)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next section, we will be reading in the table that has all the reporting areas, as well as the call weights. Here this table is named `RA_Centroids.dbf`. Then I create the basic objects, `areas`, `cont_dict`, `call_dict`, and `dist_dict`, which will be supplied to the subsequent p-median function. The other objects I create are idiosyncratic to this particular project. CSV files do not like the reporting area IDs (they have E's in them that are subsequently interpreted as scientific numbers), so sometimes I need a map back and forth between the `NumberId` and the `PDGrid` (the reporting area unique ID) (the `nid_map` and the `nid_invmap` objects). Finally, I need to impute some missing data for travel time in minutes, and the `pair_data` object will help me do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the Reporting Areas table, include call weights and XY coordinates\n",
    "ra_data = []\n",
    "vars = ['PDGrid','NumberId','XFeet','YFeet','Cnt_ra','Sum_Minute','Sum_Minu_1']\n",
    "\n",
    "for record in dbfread.DBF('RA_Centroids.dbf'):\n",
    "    temp = [record[i] for i in vars]\n",
    "    ra_data.append(temp)\n",
    "\n",
    "#I need to make a dictionary from this that maps the PDGrid to the NumberId value\n",
    "nid_map = {}\n",
    "nid_invmap = {}\n",
    "cont_dict = {} #contiguity dictionary\n",
    "call_dict = {} #call dictionary\n",
    "dist_dict = {} #distance dictionary\n",
    "areas = []\n",
    "pair_data = []\n",
    "for i in ra_data:\n",
    "    areas.append(i[0])\n",
    "    nid_map[i[1]] = i[0]\n",
    "    nid_invmap[i[0]] = i[1]\n",
    "    cont_dict[i[0]] = []\n",
    "    call_dict[i[0]] = i[4] #could also use weights, ra_data[-1]\n",
    "    dist_dict[i[0]] = {}\n",
    "    for j in ra_data:\n",
    "        dist_dict[i[0]][j[0]] = 9999\n",
    "        pair_data.append((i+j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the code reads in the contiguity matrix, and then adds in the missing links I mentioned earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the contiguity matrix, add in the missing links\n",
    "for record in dbfread.DBF('RA_ContMat.dbf'):\n",
    "    FromRA = nid_map[record['NUMBERID']]\n",
    "    ToRA = nid_map[record['NID']]\n",
    "    cont_dict[FromRA].append(ToRA)\n",
    "        \n",
    "#Now I have a two more missing links to append\n",
    "#1326 <-> 1333\n",
    "E1326 = nid_map[1326]\n",
    "E1333 = nid_map[1333]\n",
    "cont_dict[E1326].append(E1333)\n",
    "cont_dict[E1333].append(E1326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the code is basically necessary because I have missing minutes. First I create a panda's dataframe that includes all of the pairwise combinations between each area, named `FullPairs`. Then I add a column to this data frame that is the Euclidean distance between two points. Second, I create a dataframe that only includes the travel time in minutes were available, named `DistPairs`. I then run a linear regression predicting the travel time in minutes based on the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a193b1af08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the distance matrix\n",
    "dist_data = []\n",
    "for record in dbfread.DBF('RA_DistMat.dbf'):\n",
    "    f,t = record['Name'].split(' - ')\n",
    "    min = record['Total_Minu']\n",
    "    dist_data.append((f,t,min))\n",
    "    \n",
    "#Now creating a pandas dataframe of the pairs data\n",
    "FullPairs = pandas.DataFrame(pair_data, columns=['FromRA','FromNID','FX','FY','CF','CWF','CWF2',\n",
    "                                                 'ToRA',  'ToNID','TX','TY','CT','CWT','CWT2'])\n",
    "#Eliminate Columns I dont need\n",
    "FullPairs = FullPairs[['FromRA','FromNID','FX','FY','ToRA','ToNID','TX','TY','CT','CWT2']]\n",
    "    \n",
    "#Adding in column for Euclidean distance\n",
    "FullPairs['EuDist'] = ( (FullPairs.FX - FullPairs.TX)**2 + (FullPairs.FY - FullPairs.TY)**2 )**0.5\n",
    "    \n",
    "#Now creating a pandas dataframe of the dist data, estimate linear regression\n",
    "DistPairs = pandas.DataFrame(dist_data, columns = ['FromRA','ToRA','Min'])\n",
    "DistPairs = DistPairs.merge(FullPairs[['FromRA','ToRA','EuDist']], on=['FromRA','ToRA'], how='left')\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(DistPairs[['EuDist']],DistPairs[['Min']])\n",
    "reg.coef_, reg.intercept_\n",
    "DistPairs.plot.scatter('EuDist','Min')\n",
    "#Can see that they are highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the two measures are highly related, so imputing the missing travel minutes based on the Euclidean distances seems a reasonable approach. Distance here is in feet.\n",
    "\n",
    "The next set of code then takes the linear regression predictions, and calculates them based on the full pairwise dataset, not just the dataset with a measure of the travel time in minutes between locations. Then it creates a final variable that equals the calculated network distance where available, but if it is missing it uses the predicted value from the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now merging the two together\n",
    "MergeData = FullPairs.merge(DistPairs[['FromRA','ToRA','Min']], on=['FromRA','ToRA'], how='outer')\n",
    "#You can see some missing data in the last column\n",
    "MergeData['PredMin'] = reg.predict(MergeData[['EuDist']])\n",
    " \n",
    "def Mis(row):\n",
    "    if math.isnan(row['Min']):\n",
    "        val = row['PredMin']\n",
    "    else:\n",
    "        val = row['Min']\n",
    "    return val\n",
    "            \n",
    "MergeData['FinMin'] = MergeData.apply(Mis, axis=1)\n",
    "    \n",
    "#Now I can create the distance matrix \n",
    "#SubData = MergeData[:10]\n",
    "for index, row in MergeData.iterrows():\n",
    "    dist_dict[row['FromRA']][row['ToRA']] = row['FinMin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to run the p-median model. Using the previously defined `PMed` function, the call is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is Optimal, The total weighted travel is 196587\n"
     ]
    }
   ],
   "source": [
    "res1 = PMed(Ar=areas, Di=dist_dict, Co = cont_dict, Ca = call_dict, \n",
    "            Ta = 12, In = 0.10, Th = 100) #This high of threshold is no threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only takes a few minutes to run with just over 300 areas. (Some experiments I have done 480,000 decision variables in two days, but likely the nature of the data, how many areas to partition, etc. make a difference.) With 325 areas and no threshold to eliminate some source/destination pairs, there is a total of 325^2 = 105,625 decision variables.\n",
    "\n",
    "Now the final step is to save the results. Unfortunately as I mentioned, the E's in the reporting area IDs are causing problems reading the CSV files in subsequent software, so I replace the PDGrid variable with the NumberId variable, then export the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This only takes a few minutes, unfortunately the E values in the RA Ids cause problems, will replace them with the NumberId\n",
    "res1_edit = []\n",
    "for i in res1:\n",
    "    s = nid_invmap[i[0]]\n",
    "    t = nid_invmap[i[1]]\n",
    "    res1_edit.append( (s,t,i[2],i[3],i[4]) )\n",
    "\n",
    "#Now save to a csv file\n",
    "with open(\"PMed_Results1.csv\", \"w\", newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([('source','dest','dist','calls','cost')] + res1_edit)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the results using ArcGIS\n",
    "\n",
    "Now we can map the resulting clustered areas using ArcGIS. Import the CSV file into ArcMap. Then right click on the ReportingAreas layer (the polygons), and select Joins and Relates -> Joins.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/Clk-2xdBZykJ5N7LU-P2C0NDMO0zgDM2KAx6UlC234Sc51SGZUuDaD4LdSirqx2cSLgqVoLPiQx0ybOqXZvIj_dzeeoNt5P6X4w32KAjACzZIiFZmN22evQZypOO7gArdP-tMDHUt6Av0EloKtfdwkNKTOE6EcyWMWsBRh2zwAvvNYCHkqBeC5cHOhdEEnnEkLARQ8I1T4xKlUCGKmGcdJvifpteI43tORZlBa0uj7BzE4V1GXJM_NLaCYyUGXuP8zurWNlDNFsTe9_343X4SMIcizwQZqBH6sFt1p2iNXO1LFMmfqJBVaRj8Kq7p5UoSwjk4x_8eNbaBF5_Kj8yI5vmJGhtyYP52hHGC8vQjgUANTmzTpZzszH_zJmHpm7ldkj93sy-YUu4YGZj0RknwLVPGIEh96vJU43Lqtc1o2BiUdvoLRB_aExTeqokvorU1Le2r77aVwKpoh6xGXRqgV08fnq09k94aLUa1dIhE8l-Fp0cr0_OzLUSIr7YQvn1G-jUn9wNQt5O4S67NmIud2fyVYz5xrJWQA8GjZuEVaHGREZrTx8suEI2JFuHBf4t6GFIIDgoZMEntKMHKzrz3zqc5wVs0rHOvUtO8dMLPw=w796-h696-no)\n",
    "\n",
    "In the dialog select to join attributes from table, specify the join field as the `NumberId` field, specify the csv file we just created to join to, and then make sure to select the `dest` column as what to join. Once that is all entered, go ahead and click OK.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/Ik3x6YPoXRXMw-kyzgxG8XZ9LhwWFv8swkxNGsmI5i1fJFQ_dWjgSrWrCnKHDW6k27fDCaB1wY_HMrsZxsLZENB-wETfroGFQGJBnQszBaOQzKoGhA7O6ttj6OuKh6Pb4OBfSFE5tcoI7MH8uDuL3TAYNonxnlW-08OizgAQBcf__b5dTUdGSnVhxLlXV5fpVQofJPTk8E47VzsFQ0yKl0blKE0Jmz-fAb4OS5-HiynEJwM78TmWOE6K4qDTMVc_lT_TlnvahXYp4OvA5O1GFQHu_7AUE889pq5wh9LmWCb9p8moUR5C7v2dL6M6ReZxxnBxXe-Nr2bzeQ9Dg1LP6XFYYbhV3NLd8lF-RuDoI9BXtz5wrZyGpNLas27SoU9h3elPpwzWjCCAoSERmaHag7KZLxqyi81Ot2TKDT8jw_tJE7iVJ2Uk8iLXCyWnUZug4Ns95Va0FnOm-sd1bwHADKrWLB87zVcLCsHcNM1joijxk0Ay8sgKry8W9GYZqDG_hHeNMAkSEu0PvPh63sMVmxRIshgZRQx98dhmCQIGqLSACXH6D_vAzo1ed7G5Ue7C2jiNs5t9-C-v-c82YwgWDp5NiXb7vRsOGpbAIXs5dA=w412-h602-no)\n",
    "\n",
    "Now right click on the ReportingAreas layer in the table of contents and select Properties.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/EQl-SDhgqeXVUYZt__Z-Yn7MO2p5Dz1seAmO9ICs-hDO7PbR7n_cA_q522s5H8_G1DR4M1nPdcnJuKNNhyG2hmdiXfuWzsqYHaQoG0y2CWGrGNUdJvx1NsZX1bIii1qC1Zotz9syTuxkdkw6Cs5o_dSbjdH-g6Ob79VD3dhA1H03tr3pB3FhMUpcUaqAXGCzU8QoKH5ei_DtgoWMvupYPlMQF21qGKIPaDA8n2ngyrviGcebXHWQofQ4RwniGs9wy58htdzSJjAcl_8PfPH8Ai_Rj27oZXSxEnboyHrYUdfn3jSemH6iOC_dYBuY014xS_cP6rBvq-3PbqroQMFf765RsumS9f4oRh0KK4stJJDY-4rtm8FXGp8w3MFXTlQj59T0yVU5-zUxJs9CjTklIunKgnKRgTCoycuuvHzcuIcb2xVlY1MtEJyQfuqy3kRElHQJpPqDdSlpHe1-DLJp-N_HAbN1QaYWInYWJfIjE3ryWeTVXmD-2qgZkDvQIftGBXDxCqKnf2TeHDfZwdDRTFSbGjRH9eNNwLg8mwwdukNgXJUH_6sN3_xmLPdACKQH-lzULVuTr2yzdU1LCpNFVuAJhHDchXmcJpl1gOfjXg=w512-h740-no)\n",
    "\n",
    "Now we are going to change the color of the reporting area based on its source area - which uniquely defines the resulting areas. Go to the Symbology tab, select Categories -> Unique Quantities in the left hand porting, and then click Add All Values. These variables are then the unique source reporting areas. You can use any qualitative palette you want to vizualize the results. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/wFevpB-9uxhHob7zpnzBv6aV8snfHMmROXUlC-GeGiJcMUgg1kLL93vhnWecIyiDsHGlc-bPFazGlEGiDpncloTRr4E_t2bQOZWDRzvJxnNh9G4kG9ua4DwVkipJxCfoEjACys3Z9ugm1A0dM8uyE98xUkHIQwgpVyTEEQ1_Kzpf9CKDacUpic_VZnHfVboYUo7LuRviU58DnxrvBr7z9bVaULHPaG9Jn_o62VBzAT6QWBcaDB7497ssX60NhOmEFxj6gMqWeMU92P7_jk7TCfE6-XTef9tYUYHSnhgvQudA7Z2kCA48n66ku7RoZOs71TYfMVFOzvKXHUpnGoqvcI0729_A4KAP42hDHe79l_EGqjfsDRqOPhswjCMPjTAWFQE0uBNXDUfoa7kS_B6YTQvXhfG_MEHJ8Z2ZYihYP9D-C_APlY9CEJWKHgVNvu1z5sXxZRrT6HO879iw5xbHSMS2YpUmdXX4JOvPzCr1ECa7UeqmoP6RX-aYyLkSjoC8TI6sFREBMetgbowqBc-r5oEshcA9MylffAdWa3lC6Bo-xzstO-zH68PjqVxniPsx9UkcZyOlnIItOR9Uh2Q3eVSkW2HDttuB-QQhsGXI6A=w656-h517-no)\n",
    "\n",
    "After you click ok, you will then see your map, here is a screenshot. \n",
    "\n",
    "![](https://lh3.googleusercontent.com/2rMt4P1pmqmUmey7MNr2xLHokiCSsvjsadYGFONS3Lc1nyOITql465r_XMA9crHRh3KhHWDrGBa8WBXncOG0WzhTDVA2GtFE2HmPpQ7VGVIZdg4hbwAK6ev2mnPpBeGyPzS0a10xoh5Ep-12HXIQdUsk44bhW3rdVdwA-jgp6Po7y96y4uBfa3AClidB4jUwD5mPGWZkXKPKUephumcPWDAOc4K6EzqkVovhc0nmoDUkFAUM_rpiL7CLb7G_-IRUTWiDK2zN9GYeYff4Lz1m8RxA3soxykZa8AjhvMQNRfEO_5SvD65nmyVk6wxCBWG1fbJYT87EM_SavIO1khLt2JEA4Z_3k66fgCThfqsLPLcrcaBUwFJLJoXgge63she_RANuC_GqWbn-6JbPin6UnMoPb1xPIMIt3hxU_n_I8GkmqRC0kwcSOyItrhmgHG9K7SfziaH5ozyWM73nnxsB8pSTwGeOHDWpCvYGcWlRH_6EKlgIhBVw6yH8hfd6rf5N7n7GdHF1qkHvhOd4063NvdpbfR5wVIpV6DKc6HY1P8xi5t73N-I7Xc40LdKp1a1nokHVXTFxGdRwgfd7J8i_VqJlY6sL4YnrTplEQzKKPw=w672-h843-no)\n",
    "\n",
    "You can see I have two seperate clusters of areas that appear problematic circle in red, in that they are discontinuous with the assigned area (they still meet the constraints though, due to circular areas of being closer in terms of network distance due to highways, but being farther away in geographical distance). It ends up that these areas have zero calls for service though, so are entirely arbitrary as to what location they are assigned in the end. Changing their assigned source will not change the overall weighted travel.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/vsZW8cdJbHAl6TrQ67Ljg8MNXwibUl_YhGls5q96NgqvvCLZQcaQ8U9xJUFZUACNSE4dvxoCldclWTdURLO8hFsk3zk1sOMEnvdDg1y3Sc3mgyHW8ZpgZQZHwKgBmqpz6X6V_buR-26ENbZKyB7DY5nLvkiuplGaQJOX8XTIlMS1zNxdDH23H5t4ft3a5gt4xbr0RrCksL3WIDHSpd9l2dpCYEtxJml2830UPoChXBSYzmHwSiy-n20CTXTrDqkElAaauuir5yl87-qkl6YkySaQefjDDfi47SHlbnG5k4ZMX1tEChzamLExuSj2uuNIIrFzyElUI8I4KToKNPam8ucU1iDuEx-HX_E8O4Yu6VSHsJ_6QhyLWnBPunkUg_YEZR0khYVYRfQR_5BMSX4uLGRIHyMHIYyCl_aXrOBnuU5Wdy5Euzwb4X7wMGof5BomL8tiXNrYVDiYMMMEJFTcxOJDMjs8ux4Y0G15uksE_KakcGsUqj6vg9f9sO0ly7ztyXU6_NHKCszDPKwMtwboQCggiECBCQ__lHgOjuYK19GbpOR6Ze8p9R48Zz9zvvlCq_7PpiwQwGpeP6V_O_3nlUx68KQOLaBX5qf2_4kr2Q=w1117-h838-no)\n",
    "\n",
    "I will show how to edit those zero areas to more appropriate assignments. Here to edit the file, I first export the reporting areas layer to a new shapefile. Right click on the reporting areas file in the table of contents, and then click Data -> Export Data.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/yX6l32_MVjhPaSVMUHBmlTd0bi5AFHDjt27KWug5XmJ-gOA-LIHLHav3yDF3qAJ6aB0h2jbGTbh1-PeSki-XXHqXp_GAdD-Ap63colAIdMrWC1bPsKR4RKSXnyjHBSiXsD3wHCffD_txxSATmu-In471fNWdoDWZFXOjnpkvZfTot2kAv5zS3oneInFBGlDwEJKcBTG_JvBxYT2PcZdLMjkzPTZIlK0JKOq2jNKb0E_EQVYkl93uRoKjZNzjqfJXlyrxLk2RtU4-gDzJvK5RdcGQj_-sWAvqbJrr5_4ZRUE4JLnjK_6s6l0uNY_YNXxqZxFygDPFB9loK6vtgZE7KhP1Hwz3oYOkiiJKig1KQouANhQuqWRpFO0yKKsnj0agI8cqdX_WusAI552NLBDB0ZnHxEuGLAWwxcU3cZiofTtlf-WDJ_1ykoGZdsyf2lrZ58R-pR-KFGXDmZLxSbHp6tI-bHJM6VZPc4oMueXraX9Au8WWMXEi-pFkBe1d0FicWtvJJ28LK1BYjdBrna-uutEeORuNJBlWF3cwOvVL52obMhzmfyLQ1yeHhBxE8GrhEWIbQhRaR2HrDun78lf_gw5yhOAY_UK3pJIcBYqDvg=w748-h752-no)\n",
    "\n",
    "Here I save the file as `Assigned_ReportingAreas`, and add that new shapefile back into the map. Then open up that attribute table, and create a new variable named `FinSource`. Right click on the field and then select Calculate Field. In the resulting dialog then calculate this field based on the original source field.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/AEdriXN9evANlhdYxJ-mur4xGNX8FopaL-Xa-labfY0LAayxwf5fugDpJ3oJRYIqkVepF9kKhjshIG_kn0nWhlAcaVf_eUitPdnbT6ejFpdnw78SEDLvUv4-EyOGZgARZofz1cZ4t307LQY7I0Xz5gbrr2f6QZ1OGVN1ZJHrhfdntmbdquVHOHIqIsBjFMiFtONMVb10wUFwUD-fDovnR2RcFnsKPE5wcSRVj4qgW2Pq9Mi-OAH2jwi-xHxzfdeoT8sZjIszCHXJk9IVQZEhLTTfcqBgw2_hSxMmwrC07dnyRnhGGBIgNdenlfRmnjjXUXe1VA_NJQJdEgftJD48zp641J0GXE7Oyj9gO267ufPsZAXIcSCB06py5muz-SLrM79QeDLp9bQrHaEgU2dxM9pih9bh_0HZjIZ2k7Kmp-kYiL-uszCEjPlkkZjtGBxNz-IXQJJX5d0GwCCtZa5jWVIJMT2qEuu_rNHzsxgM54Eu4K5WRvy-qsLls5HsYCo9w9Ir56qCxLxa9iBzSfcnbv6mBfAi20qpY_FJztOHedt9Dh3b301y1PsWWfKWrcfmXCXdRCut0ndeCTGIZRD_bFpCc53MYVhkOclhOy6ovg=w463-h562-no)\n",
    "\n",
    "Then to edit the particular sources you can select those observations, and then go into the field calculator again and specify the source that you want. Here I show how to change the upper reporting areas to the source of 1277, instead of the not adjacent area of 1094. This works because field calculations are only done on selected records.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/ClGuSlcdqWN9dGDz1lycmXMglRmxZTTKkQA0UCmOuWiyo1EloDOWDHG29o0zGyYSfURxsXCQqDV48NIIP-X-4i_Ja-4msvb76DkgEKuIjURiKo7TTyE8rNGfTX6GydNwj8k13KC_AS3-rUL75pktma_g6cvp9NS6t_0uuqPIeFMMSJCwJXrN3T8Jrjb96LXR-MyNj-jGYbRXjQxoxroO1eJvi1tseJWs5NBuw7bcfly4ZFFEiqT8KKW2nHvpcAC882ZPvqMEhU9JQjDP9gIaukLITMj9OIHHIM-nGGzrkesRNkrrPbY0XXG95_N_0-DY_pbvqAZcZV9UOtzzQK4S1nl5c7aBWCQbyG0z4R2B76XEuOnC-AShTEdveDASYkLnc7gDAUybOJQFMUU6g5r7b1e5W3-3ktYLREWR_zqjj6ad623oJgGH119aSLklt__p_XI60Gw0gnciNjRjfvueAA4i8hw1DeiT9PDEPUoq1lSxvUFqRFjs_KP-Q0VdN68rbDBZl4t6nkSw04FnBxE7tlNbONFldjmGLQ60LwyqV-rzujakoDhPXR0KmLK_bKekoGu_F7a926NlpWOHhsF2P4Q-e5N0Vpa_0Nso97pudw=w1582-h822-no)\n",
    "\n",
    "You can subsequently do this for all of the remaining areas where the calls are zero. This will not change the estimate of the weighted travel if you only change locations that have zero calls. \n",
    "\n",
    "You may want to change areas though to make the resulting areas \"look nicer\", or conform to other aspects not specified in the model. You would do those changes the same way, but that will change the overall weighted travel, and can potentially increase the inequality in the solution. Starting from this point though is easier than starting from scratch.\n",
    "\n",
    "# Extra -- Python Notes\n",
    "\n",
    "If you do not understand how to execute the code written - you at first need to familiarize yourself with the python programming language. \n",
    "\n",
    "Personally I use the [Anaconda distribution of python](https://conda.io/docs/user-guide/install/index.html). The reason for this is that it is very easy to install subsequent packages. If you are using Anaconda, all you have to do is on the command line type `conda install pulp` and it will install the pulp library. (Or if that does not work, simply google \"conda install pulp\", and you will see you can alternatively [install it like](https://anaconda.org/conda-forge/pulp) `conda install -c conda-forge pulp`.)\n",
    "\n",
    "Other software frequently has python bindings -- like ArcGIS. You may use that version of python, but you will need to install pulp another way. I have always had problems getting `pip` to work on my windows machines, hence why I like using conda. \n",
    "\n",
    "This tutorial is written using a Jupyter notebook, and I like to develop python code using the Spyder editor that comes with conda as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
